<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url(https://themes.googleusercontent.com/fonts/css?kit=W8Jl-jxzzEnyRbrIe94pac0zaykDSyyRzuugVKVeSl4);ol{margin:0;padding:0}table td,table th{padding:0}.c0{border-right-style:solid;padding:4.5pt 4.5pt 4.5pt 4.5pt;border-bottom-color:#000000;border-top-width:0pt;border-right-width:0pt;border-left-color:#000000;vertical-align:middle;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;width:20.6pt;border-top-color:#000000;border-bottom-style:solid}.c2{background-color:#ffffff;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier";font-style:normal}.c10{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c12{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier";font-style:normal}.c8{margin-left:6.4pt;padding-top:6.4pt;padding-bottom:6.4pt;line-height:1.0;text-align:left;margin-right:6.4pt;height:10pt}.c1{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c33{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c9{padding-top:0pt;padding-bottom:0pt;line-height:1.2858099937438965;text-align:left;height:10pt}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:10pt}.c27{color:#000000;text-decoration:none;vertical-align:baseline;font-size:18pt;font-style:normal}.c21{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left;height:18pt}.c25{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left;height:10pt}.c7{border-spacing:0;border-collapse:collapse;margin-right:auto}.c18{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:right}.c22{padding-top:0pt;padding-bottom:0pt;line-height:1.2858099937438965;text-align:left}.c5{padding-top:4.5pt;padding-bottom:4.5pt;line-height:1.0;text-align:left}.c15{padding-top:4.5pt;padding-bottom:4.5pt;line-height:1.0;text-align:right}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.25;text-align:left}.c24{padding-top:9pt;padding-bottom:16pt;line-height:1.0;text-align:left}.c23{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c31{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c3{background-color:#ffffff;font-family:"Courier";font-weight:700}.c16{margin-left:6.4pt;margin-right:6.4pt;height:10pt}.c4{background-color:#ffffff;font-family:"Courier";font-weight:400}.c26{color:inherit;text-decoration:inherit}.c28{font-weight:700;font-family:"Courier"}.c17{margin-left:6.4pt;margin-right:6.4pt}.c32{font-weight:700;font-family:"Arial"}.c19{margin-right:15pt}.c30{height:10pt}.c13{font-style:italic}.c20{height:0pt}.c29{font-style:normal}.title{padding-top:24pt;color:#000000;font-weight:700;font-size:36pt;padding-bottom:6pt;font-family:"Arial";line-height:1.2858099937438965;page-break-after:avoid;text-align:left}.subtitle{padding-top:18pt;color:#666666;font-size:24pt;padding-bottom:4pt;font-family:"Georgia";line-height:1.2858099937438965;page-break-after:avoid;font-style:italic;text-align:left}li{color:#000000;font-size:10pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:10pt;font-family:"Arial"}h1{padding-top:12pt;color:#000000;font-weight:700;font-size:24pt;padding-bottom:12pt;font-family:"Arial";line-height:1.2858099937438965;text-align:left}h2{padding-top:11.2pt;color:#000000;font-weight:700;font-size:18pt;padding-bottom:11.2pt;font-family:"Arial";line-height:1.2858099937438965;text-align:left}h3{padding-top:12pt;color:#000000;font-weight:700;font-size:14pt;padding-bottom:12pt;font-family:"Arial";line-height:1.2858099937438965;text-align:left}h4{padding-top:12.8pt;color:#000000;font-weight:700;font-size:12pt;padding-bottom:12.8pt;font-family:"Arial";line-height:1.2858099937438965;text-align:left}h5{padding-top:12.8pt;color:#000000;font-weight:700;font-size:9pt;padding-bottom:12.8pt;font-family:"Arial";line-height:1.2858099937438965;text-align:left}h6{padding-top:18pt;color:#000000;font-weight:700;font-size:8pt;padding-bottom:18pt;font-family:"Arial";line-height:1.2858099937438965;text-align:left}</style></head><body class="c31 doc-content"><p class="c9"><span class="c33 c29"></span></p><p class="c9"><span class="c33 c29"></span></p><p class="c9"><span class="c29 c33"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;google.colab </span><span class="c3">import</span><span class="c4">&nbsp;drive<br>drive</span><span class="c3">.</span><span class="c2">mount(&#39;/content/drive&#39;)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&quot;/content/drive&quot;, force_remount=True).<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4 c13">#IMPORTS</span><span class="c4"><br><br></span><span class="c3">import</span><span class="c4">&nbsp;tensorflow </span><span class="c3">as</span><span class="c4">&nbsp;tf<br></span><span class="c3">import</span><span class="c4">&nbsp;numpy </span><span class="c3">as</span><span class="c4">&nbsp;np<br></span><span class="c3">import</span><span class="c4">&nbsp;sys<br></span><span class="c3">import</span><span class="c4">&nbsp;sklearn </span><span class="c3">as</span><span class="c4">&nbsp;sk<br></span><span class="c3">import</span><span class="c4">&nbsp;pandas </span><span class="c3">as</span><span class="c4">&nbsp;pd<br></span><span class="c3">import</span><span class="c4">&nbsp;json<br></span><span class="c3">import</span><span class="c4">&nbsp;csv<br></span><span class="c3">import</span><span class="c4">&nbsp;random<br></span><span class="c3">from</span><span class="c4">&nbsp;sklearn.feature_extraction.text </span><span class="c3">import</span><span class="c4">&nbsp;TfidfVectorizer,CountVectorizer<br></span><span class="c3">import</span><span class="c2">&nbsp;requests<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4 c13">#Removes all records with categorical values that only appear in training or test data</span><span class="c4"><br>input_file1 </span><span class="c3">=</span><span class="c4">&nbsp;&quot;/content/drive/MyDrive/Assignment_2/UNSW_NB15_training-set.csv&quot;<br>output_file1 </span><span class="c3">=</span><span class="c4">&nbsp;&quot;/content/drive/MyDrive/Assignment_2/cleansed_training_set_1.tsv&quot;<br>dfTraining </span><span class="c3">=</span><span class="c4">&nbsp;pd</span><span class="c3">.</span><span class="c4">read_csv(input_file1, sep</span><span class="c3">=</span><span class="c4">&#39;,&#39;, encoding</span><span class="c3">=</span><span class="c4">&#39;utf-8&#39;)<br><br>input_file2 </span><span class="c3">=</span><span class="c4">&nbsp;&quot;/content/drive/MyDrive/Assignment_2/UNSW_NB15_test-set.csv&quot;<br>output_file2 </span><span class="c3">=</span><span class="c4">&nbsp;&quot;/content/drive/MyDrive/Assignment_2/cleansed_test_set_1.tsv&quot;<br>dfTest </span><span class="c3">=</span><span class="c4">&nbsp;pd</span><span class="c3">.</span><span class="c4">read_csv(input_file2, sep</span><span class="c3">=</span><span class="c4">&#39;,&#39;, encoding</span><span class="c3">=</span><span class="c4">&#39;utf-8&#39;)<br><br>selectedColumns </span><span class="c3">=</span><span class="c4">&nbsp;[&#39;proto&#39;, &#39;service&#39;, &#39;state&#39;, &#39;attack_cat&#39;]<br><br>dfTraining[&#39;wantedColumns&#39;] </span><span class="c3">=</span><span class="c4">&nbsp;dfTraining[selectedColumns]</span><span class="c3">.</span><span class="c4">apply(tuple, axis</span><span class="c3">=</span><span class="c4">1)<br>dfTest[&#39;wantedColumns&#39;] </span><span class="c3">=</span><span class="c4">&nbsp;dfTest[selectedColumns]</span><span class="c3">.</span><span class="c4">apply(tuple, axis</span><span class="c3">=</span><span class="c4">1)<br><br>dfTraining_Unique </span><span class="c3">=</span><span class="c4">&nbsp;set(dfTraining[&#39;wantedColumns&#39;])<br>dfTest_Unique </span><span class="c3">=</span><span class="c4">&nbsp;set(dfTest[&#39;wantedColumns&#39;])<br><br>diffTrainingVals </span><span class="c3">=</span><span class="c4">&nbsp;dfTraining_Unique </span><span class="c3">-</span><span class="c4">&nbsp;dfTest_Unique<br>diffTestVals </span><span class="c3">=</span><span class="c4">&nbsp;dfTest_Unique </span><span class="c3">-</span><span class="c4">&nbsp;dfTraining_Unique<br><br>extraCleansed_training </span><span class="c3">=</span><span class="c4">&nbsp;dfTraining[</span><span class="c3">~</span><span class="c4">dfTraining[&#39;wantedColumns&#39;]</span><span class="c3">.</span><span class="c4">isin(diffTrainingVals)]<br>extraCleansed_test </span><span class="c3">=</span><span class="c4">&nbsp;dfTest[</span><span class="c3">~</span><span class="c4">dfTest[&#39;wantedColumns&#39;]</span><span class="c3">.</span><span class="c4">isin(diffTestVals)]<br><br>extraCleansed_training </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_training</span><span class="c3">.</span><span class="c4">drop(columns</span><span class="c3">=</span><span class="c4">[&#39;wantedColumns&#39;])<br>extraCleansed_test </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_test</span><span class="c3">.</span><span class="c4">drop(columns</span><span class="c3">=</span><span class="c4">[&#39;wantedColumns&#39;])<br><br>extraCleansed_training</span><span class="c3">.</span><span class="c4">to_csv(output_file1, sep</span><span class="c3">=</span><span class="c4">&quot;\t&quot;)<br>extraCleansed_test</span><span class="c3">.</span><span class="c4">to_csv(output_file2, sep</span><span class="c3">=</span><span class="c2">&quot;\t&quot;)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4 c13">#Removes all rows with missing values, makes two new cleansed files &#39;cleansed_test_set_2.tsv&#39; and &#39;cleansed_training_set_2.tsv&#39; and their dataframes extraCleansed_training and extraCleansed_testing</span><span class="c4"><br></span><span class="c4 c13">#Two new dataframes are created containing the null values removed from the other dataframes, these dataframes are named null_only_training_df and null_only_testing_df</span><span class="c4"><br>input_file </span><span class="c3">=</span><span class="c4">&nbsp;&quot;/content/drive/MyDrive/Assignment_2/cleansed_training_set_1.tsv&quot;<br>output_file </span><span class="c3">=</span><span class="c4">&nbsp;&quot;/content/drive/MyDrive/Assignment_2/cleansed_training_set_2.tsv&quot;<br>df </span><span class="c3">=</span><span class="c4">&nbsp;pd</span><span class="c3">.</span><span class="c4">read_csv(input_file, sep</span><span class="c3">=</span><span class="c4">&#39;\t&#39;, encoding</span><span class="c3">=</span><span class="c4">&#39;utf-8&#39;)<br><br><br>removal_value </span><span class="c3">=</span><span class="c4">&nbsp;&#39;-&#39;<br>extraCleansed_training </span><span class="c3">=</span><span class="c4">&nbsp;df[df[&#39;service&#39;] </span><span class="c3">!=</span><span class="c4">&nbsp;removal_value]<br>null_only_training_df </span><span class="c3">=</span><span class="c4">&nbsp;df[df[&#39;service&#39;] </span><span class="c3">==</span><span class="c4">&nbsp;removal_value]<br><br>extraCleansed_training </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_training</span><span class="c3">.</span><span class="c4">drop(&#39;Unnamed: 0&#39;,axis</span><span class="c3">=</span><span class="c4">1)<br>extraCleansed_training </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_training</span><span class="c3">.</span><span class="c4">drop(&#39;id&#39;,axis</span><span class="c3">=</span><span class="c4">1)<br>extraCleansed_training </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_training</span><span class="c3">.</span><span class="c4">reset_index(drop</span><span class="c3">=True</span><span class="c4">)<br>null_only_training_df </span><span class="c3">=</span><span class="c4">&nbsp;null_only_training_df</span><span class="c3">.</span><span class="c4">drop(&#39;Unnamed: 0&#39;,axis</span><span class="c3">=</span><span class="c4">1)<br>null_only_training_df </span><span class="c3">=</span><span class="c4">&nbsp;null_only_training_df</span><span class="c3">.</span><span class="c4">drop(&#39;id&#39;,axis</span><span class="c3">=</span><span class="c4">1)<br>null_only_training_df </span><span class="c3">=</span><span class="c4">&nbsp;null_only_training_df</span><span class="c3">.</span><span class="c4">reset_index(drop</span><span class="c3">=True</span><span class="c4">)<br><br>extraCleansed_training</span><span class="c3">.</span><span class="c4">to_csv(output_file, sep</span><span class="c3">=</span><span class="c4">&quot;\t&quot;)<br><br>input_file </span><span class="c3">=</span><span class="c4">&nbsp;&quot;/content/drive/MyDrive/Assignment_2/cleansed_test_set_1.tsv&quot;<br>output_file </span><span class="c3">=</span><span class="c4">&nbsp;&quot;/content/drive/MyDrive/Assignment_2/cleansed_test_set_2.tsv&quot;<br>df </span><span class="c3">=</span><span class="c4">&nbsp;pd</span><span class="c3">.</span><span class="c4">read_csv(input_file, sep</span><span class="c3">=</span><span class="c4">&#39;\t&#39;, encoding</span><span class="c3">=</span><span class="c4">&#39;utf-8&#39;)<br><br>removal_value </span><span class="c3">=</span><span class="c4">&nbsp;&#39;-&#39;<br>extraCleansed_test </span><span class="c3">=</span><span class="c4">&nbsp;df[df[&#39;service&#39;] </span><span class="c3">!=</span><span class="c4">&nbsp;removal_value]<br>null_only_testing_df </span><span class="c3">=</span><span class="c4">&nbsp;df[df[&#39;service&#39;] </span><span class="c3">==</span><span class="c4">&nbsp;removal_value]<br><br>extraCleansed_test </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_test</span><span class="c3">.</span><span class="c4">drop(&#39;Unnamed: 0&#39;,axis</span><span class="c3">=</span><span class="c4">1)<br>extraCleansed_test </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_test</span><span class="c3">.</span><span class="c4">drop(&#39;id&#39;,axis</span><span class="c3">=</span><span class="c4">1)<br>extraCleansed_test </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_test</span><span class="c3">.</span><span class="c4">reset_index(drop</span><span class="c3">=True</span><span class="c4">)<br>null_only_testing_df </span><span class="c3">=</span><span class="c4">&nbsp;null_only_testing_df</span><span class="c3">.</span><span class="c4">drop(&#39;Unnamed: 0&#39;,axis</span><span class="c3">=</span><span class="c4">1)<br>null_only_testing_df </span><span class="c3">=</span><span class="c4">&nbsp;null_only_testing_df</span><span class="c3">.</span><span class="c4">drop(&#39;id&#39;,axis</span><span class="c3">=</span><span class="c4">1)<br>null_only_testing_df </span><span class="c3">=</span><span class="c4">&nbsp;null_only_testing_df</span><span class="c3">.</span><span class="c4">reset_index(drop</span><span class="c3">=True</span><span class="c4">)<br><br>extraCleansed_test</span><span class="c3">.</span><span class="c4">to_csv(output_file, sep</span><span class="c3">=</span><span class="c2">&quot;\t&quot;)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">extraCleansed_training</span><span class="c3">.</span><span class="c4">tail()<br>extraCleansed_test</span><span class="c3">.</span><span class="c2">tail()<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14 c19"><span class="c12"></span></p><p class="c14 c19"><span class="c12"></span></p><a id="t.0e750bb9b3bdf3b7e509417238e24f04e659550b"></a><a id="t.0"></a><table class="c7"><thead><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c25"><span class="c12"></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dur</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">proto</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">service</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">state</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">spkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dpkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">rate</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sttl</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_dst_sport_ltm</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_dst_src_ltm</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">is_ftp_login</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_ftp_cmd</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_flw_http_mthd</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_src_ltm</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_srv_dst</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">is_sm_ips_ports</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">attack_cat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">label</span></p><p class="c22 c19"><span class="c1">&nbsp;</span></p></td><tbody></tbody></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35150</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.818276</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">http</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68199</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">612</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">89.211954</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Normal</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35151</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.825170</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">http</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68199</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">612</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">88.466621</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Normal</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35152</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.874942</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">http</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68199</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">612</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">83.434101</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Normal</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35153</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.740231</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">http</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68199</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">612</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">98.617867</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Normal</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35154</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.910248</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">ftp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">62</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">20</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68109</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1026</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">42.402870</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">28</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Normal</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr></thead></table><p class="c19 c24"><span class="c12">5 rows &times; 44 columns</span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">null_only_training_df</span><span class="c3">.</span><span class="c2">tail()<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14 c19"><span class="c12"></span></p><p class="c14 c19"><span class="c12"></span></p><a id="t.6bf56907f4f6bc707e8d0fcfc1347fc0b45acfbb"></a><a id="t.1"></a><table class="c7"><thead><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c25"><span class="c12"></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dur</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">proto</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">service</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">state</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">spkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dpkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">rate</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sttl</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_dst_sport_ltm</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_dst_src_ltm</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">is_ftp_login</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_ftp_cmd</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_flw_http_mthd</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_src_ltm</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ct_srv_dst</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">is_sm_ips_ports</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">attack_cat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">label</span></p><p class="c22 c19"><span class="c1">&nbsp;</span></p></td><tbody></tbody></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">93937</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.653375</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">-</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">564</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">354</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">26.018748</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">18</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Reconnaissance</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">93938</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.695566</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">-</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">564</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">354</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">24.440528</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Reconnaissance</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">93939</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.962856</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">-</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">24</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1256</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">59374</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">86.201883</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">62</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Exploits</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">93940</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3.719110</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">-</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">66</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">340</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3086</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">426483</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">108.897021</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">62</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Exploits</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">93941</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.505762</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">tcp</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">-</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">FIN</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">620</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">354</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">33.612649</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">Shellcode</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td></tr></thead></table><p class="c24 c19"><span class="c12">5 rows &times; 44 columns</span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9 c19"><span class="c12"></span></p><h2 class="c21 c19"><span class="c27 c32"></span></h2><p class="c19 c23"><span>To be or not to be, To be catogatized or not to be catogorized</span><span class="c29"><a class="c26" href="#id.gjdgxs">&para;</a></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4 c13">#After unique categorical value removed and empty rows removed</span><span class="c4"><br>extraCleansed_training[&#39;proto&#39;] </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_training[&#39;proto&#39;]</span><span class="c3">.</span><span class="c2">map({&#39;tcp&#39;: 1, &#39;udp&#39;: 0})<br><br><br>extraCleansed_training[&#39;proto&#39;]<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14"><span class="c12"></span></p><p class="c11"><span class="c6">0 &nbsp; &nbsp; &nbsp; &nbsp;1<br>1 &nbsp; &nbsp; &nbsp; &nbsp;1<br>2 &nbsp; &nbsp; &nbsp; &nbsp;1<br>3 &nbsp; &nbsp; &nbsp; &nbsp;1<br>4 &nbsp; &nbsp; &nbsp; &nbsp;1<br> &nbsp; &nbsp; &nbsp; &nbsp;..<br>80565 &nbsp; &nbsp;0<br>80566 &nbsp; &nbsp;0<br>80567 &nbsp; &nbsp;0<br>80568 &nbsp; &nbsp;0<br>80569 &nbsp; &nbsp;0<br>Name: proto, Length: 80570, dtype: int64</span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">extraCleansed_training</span><span class="c3">.</span><span class="c4">tail()<br>print(extraCleansed_training</span><span class="c3">.</span><span class="c4">columns)<br><br></span><span class="c3">def</span><span class="c4">&nbsp;get_unique_values(df, column_name):<br> &nbsp; &nbsp;print(&#39;for col:&#39;</span><span class="c3">+</span><span class="c4">col</span><span class="c3">+</span><span class="c4">&#39;, we have&#39;)<br> &nbsp; &nbsp;</span><span class="c3">return</span><span class="c4">&nbsp; df[column_name]</span><span class="c3">.</span><span class="c4">unique()</span><span class="c3">.</span><span class="c4">shape </span><span class="c3">if</span><span class="c4">&nbsp;column_name </span><span class="c3">in</span><span class="c4">&nbsp;df</span><span class="c3">.</span><span class="c4">columns </span><span class="c3">else</span><span class="c4">&nbsp;&quot;Column not found&quot;<br><br></span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;extraCleansed_training</span><span class="c3">.</span><span class="c2">columns:<br> &nbsp; print(get_unique_values(extraCleansed_training, col))<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">Index([&#39;dur&#39;, &#39;proto&#39;, &#39;service&#39;, &#39;state&#39;, &#39;spkts&#39;, &#39;dpkts&#39;, &#39;sbytes&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;dbytes&#39;, &#39;rate&#39;, &#39;sttl&#39;, &#39;dttl&#39;, &#39;sload&#39;, &#39;dload&#39;, &#39;sloss&#39;, &#39;dloss&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;sinpkt&#39;, &#39;dinpkt&#39;, &#39;sjit&#39;, &#39;djit&#39;, &#39;swin&#39;, &#39;stcpb&#39;, &#39;dtcpb&#39;, &#39;dwin&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;tcprtt&#39;, &#39;synack&#39;, &#39;ackdat&#39;, &#39;smean&#39;, &#39;dmean&#39;, &#39;trans_depth&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;response_body_len&#39;, &#39;ct_srv_src&#39;, &#39;ct_state_ttl&#39;, &#39;ct_dst_ltm&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;ct_src_dport_ltm&#39;, &#39;ct_dst_sport_ltm&#39;, &#39;ct_dst_src_ltm&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;is_ftp_login&#39;, &#39;ct_ftp_cmd&#39;, &#39;ct_flw_http_mthd&#39;, &#39;ct_src_ltm&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;ct_srv_dst&#39;, &#39;is_sm_ips_ports&#39;, &#39;attack_cat&#39;, &#39;label&#39;],<br> &nbsp; &nbsp; &nbsp;dtype=&#39;object&#39;)<br>for col:dur, we have<br>(30938,)<br>for col:proto, we have<br>(2,)<br>for col:service, we have<br>(12,)<br>for col:state, we have<br>(4,)<br>for col:spkts, we have<br>(416,)<br>for col:dpkts, we have<br>(383,)<br>for col:sbytes, we have<br>(5201,)<br>for col:dbytes, we have<br>(5328,)<br>for col:rate, we have<br>(31261,)<br>for col:sttl, we have<br>(6,)<br>for col:dttl, we have<br>(5,)<br>for col:sload, we have<br>(32007,)<br>for col:dload, we have<br>(31697,)<br>for col:sloss, we have<br>(364,)<br>for col:dloss, we have<br>(308,)<br>for col:sinpkt, we have<br>(30695,)<br>for col:dinpkt, we have<br>(30592,)<br>for col:sjit, we have<br>(31105,)<br>for col:djit, we have<br>(30851,)<br>for col:swin, we have<br>(2,)<br>for col:stcpb, we have<br>(31021,)<br>for col:dtcpb, we have<br>(31007,)<br>for col:dwin, we have<br>(2,)<br>for col:tcprtt, we have<br>(20425,)<br>for col:synack, we have<br>(19526,)<br>for col:ackdat, we have<br>(18683,)<br>for col:smean, we have<br>(1195,)<br>for col:dmean, we have<br>(1285,)<br>for col:trans_depth, we have<br>(11,)<br>for col:response_body_len, we have<br>(2376,)<br>for col:ct_srv_src, we have<br>(51,)<br>for col:ct_state_ttl, we have<br>(5,)<br>for col:ct_dst_ltm, we have<br>(49,)<br>for col:ct_src_dport_ltm, we have<br>(45,)<br>for col:ct_dst_sport_ltm, we have<br>(29,)<br>for col:ct_dst_src_ltm, we have<br>(51,)<br>for col:is_ftp_login, we have<br>(4,)<br>for col:ct_ftp_cmd, we have<br>(4,)<br>for col:ct_flw_http_mthd, we have<br>(8,)<br>for col:ct_src_ltm, we have<br>(50,)<br>for col:ct_srv_dst, we have<br>(51,)<br>for col:is_sm_ips_ports, we have<br>(1,)<br>for col:attack_cat, we have<br>(8,)<br>for col:label, we have<br>(2,)<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9 c19"><span class="c6"></span></p><h2 class="c21 c19"><span class="c27 c28"></span></h2><p class="c23 c19"><span>Our hot encoding factory</span><span class="c29"><a class="c26" href="#id.30j0zll">&para;</a></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c16"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">import</span><span class="c4">&nbsp;pandas </span><span class="c3">as</span><span class="c4">&nbsp;pd<br></span><span class="c3">def</span><span class="c4">&nbsp;hot_encode_columns(df, min_values</span><span class="c3">=</span><span class="c4">3, threshold</span><span class="c3">=</span><span class="c4">10):<br> &nbsp; &nbsp;&quot;&quot;&quot;<br> &nbsp; &nbsp;One-hot encode the columns of the DataFrame based on the specified threshold and minimum number of unique values.<br><br> &nbsp; &nbsp;Parameters:<br> &nbsp; &nbsp;- df: The input DataFrame<br> &nbsp; &nbsp;- min_values: The minimum number of unique values for a column to be considered for one-hot encoding<br> &nbsp; &nbsp;- threshold: The maximum number of unique values for a column to be considered for one-hot encoding<br><br> &nbsp; &nbsp;Returns:<br> &nbsp; &nbsp;- one_hot_columns: List of new one-hot encoded column names<br> &nbsp; &nbsp;- df: The DataFrame with the one-hot encoded columns added and original columns dropped<br> &nbsp; &nbsp;&quot;&quot;&quot;<br><br> &nbsp; &nbsp;one_hot_columns </span><span class="c3">=</span><span class="c4">&nbsp;[]<br> &nbsp; &nbsp;columns_to_drop </span><span class="c3">=</span><span class="c4">&nbsp;[]<br><br> &nbsp; &nbsp;</span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;df</span><span class="c3">.</span><span class="c4">columns:<br> &nbsp; &nbsp; &nbsp; &nbsp;unique_values </span><span class="c3">=</span><span class="c4">&nbsp;df[col]</span><span class="c3">.</span><span class="c4">unique()</span><span class="c3">.</span><span class="c4">shape[0]<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;(unique_values </span><span class="c3">&lt;</span><span class="c4">&nbsp;threshold) </span><span class="c3">&amp;</span><span class="c4">&nbsp;(min_values </span><span class="c3">&lt;=</span><span class="c4">&nbsp;unique_values):<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;one_hot_df </span><span class="c3">=</span><span class="c4">&nbsp;pd</span><span class="c3">.</span><span class="c4">get_dummies(df[col], prefix</span><span class="c3">=</span><span class="c4">f&#39;hot_{col}&#39;)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;df </span><span class="c3">=</span><span class="c4">&nbsp;pd</span><span class="c3">.</span><span class="c4">concat([df, one_hot_df], axis</span><span class="c3">=</span><span class="c4">1)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;columns_to_drop</span><span class="c3">.</span><span class="c4">append(col)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;one_hot_columns</span><span class="c3">.</span><span class="c4">extend(one_hot_df</span><span class="c3">.</span><span class="c4">columns)<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Drop the original columns that were one-hot encoded</span><span class="c4"><br> &nbsp; &nbsp;df</span><span class="c3">.</span><span class="c4">drop(columns</span><span class="c3">=</span><span class="c4">columns_to_drop, axis</span><span class="c3">=</span><span class="c4">1, inplace</span><span class="c3">=True</span><span class="c4">)<br><br> &nbsp; &nbsp;</span><span class="c3">return</span><span class="c4">&nbsp;one_hot_columns, df<br><br><br></span><span class="c4 c13"># Example usage:</span><span class="c4"><br></span><span class="c4 c13"># new_cols, new_df = hot_encode_columns(df)</span><span class="c4"><br><br><br></span><span class="c4 c13"># Example usage:</span><span class="c2"><br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9 c19"><span class="c2"></span></p><h2 class="c21 c19"><span class="c3 c27"></span></h2><p class="c23 c19"><span>Check Catogorize</span><span class="c29"><a class="c26" href="#id.1fob9te">&para;</a></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">import</span><span class="c4">&nbsp;pandas </span><span class="c3">as</span><span class="c4">&nbsp;pd<br></span><span class="c3">import</span><span class="c4">&nbsp;matplotlib.pyplot </span><span class="c3">as</span><span class="c4">&nbsp;plt<br></span><span class="c3">import</span><span class="c4">&nbsp;seaborn </span><span class="c3">as</span><span class="c4">&nbsp;sns<br><br></span><span class="c4 c13"># Assuming df is your DataFrame</span><span class="c4"><br><br></span><span class="c4 c13"># List of columns that you may want to examine further</span><span class="c4"><br>columns_to_examine </span><span class="c3">=</span><span class="c4">&nbsp;[&#39;dur&#39;, &#39;rate&#39;, &#39;sload&#39;, &#39;dload&#39;, &#39;sinpkt&#39;, &#39;dinpkt&#39;, &#39;sjit&#39;, &#39;djit&#39;, &#39;stcpb&#39;, &#39;dtcpb&#39;, &#39;tcprtt&#39;, &#39;synack&#39;, &#39;ackdat&#39;]<br><br></span><span class="c4 c13"># Loop through each column and plot a histogram</span><span class="c4"><br></span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;columns_to_examine:<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">figure(figsize</span><span class="c3">=</span><span class="c4">(10, 6))<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Using Seaborn for a more visually pleasing histogram</span><span class="c4"><br> &nbsp; &nbsp;sns</span><span class="c3">.</span><span class="c4">histplot(extraCleansed_training[col], kde</span><span class="c3">=False</span><span class="c4">, bins</span><span class="c3">=</span><span class="c4">50)<br><br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">title(f&#39;Histogram of {col}&#39;)<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">xlabel(col)<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">ylabel(&#39;Frequency&#39;)<br><br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c2">show()<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">columns_to_examine </span><span class="c3">=</span><span class="c4">&nbsp;[&#39;dur&#39;, &#39;rate&#39;, &#39;sload&#39;, &#39;dload&#39;, &#39;sinpkt&#39;, &#39;dinpkt&#39;, &#39;sjit&#39;, &#39;djit&#39;, &#39;stcpb&#39;, &#39;dtcpb&#39;, &#39;tcprtt&#39;, &#39;synack&#39;, &#39;ackdat&#39;]<br>one_hot_columns, df</span><span class="c3">=</span><span class="c4">hot_encode_columns(extraCleansed_training, threshold</span><span class="c3">=</span><span class="c2">260)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c2">df<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14 c19"><span class="c12"></span></p><p class="c14 c19"><span class="c12"></span></p><a id="t.96f2fad6225b603e18702a3eaf7655c08ad3e5b9"></a><a id="t.2"></a><table class="c7"><thead><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c25"><span class="c12"></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dur</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">proto</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">spkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dpkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">rate</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sloss</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_ct_srv_dst_51</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_ct_srv_dst_52</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Backdoor</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_DoS</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Exploits</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Fuzzers</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Generic</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Normal</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Reconnaissance</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Worms</span></p><p class="c22 c19"><span class="c1">&nbsp;</span></p></td><tbody></tbody></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.681642</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">628</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">770</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.677108</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.740179e+03</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3358.622070</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.093085</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">62</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">28</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">56329</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2212</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">42.520967</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.118251e+05</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8152.559082</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">28</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.393556</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">860</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1096</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">43.195886</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.573347e+04</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">19494.048830</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">3</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.338017</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">998</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">268</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">44.376468</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.127704e+04</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5301.508789</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">4</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.964656</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">690</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">950</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">25.915974</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.315885e+03</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">7223.300293</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80565</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000006</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">166666.660800</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">7.600000e+07</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80566</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">111111.107200</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.066666e+07</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80567</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">111111.107200</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.066666e+07</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80568</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">111111.107200</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.066666e+07</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80569</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">111111.107200</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.066666e+07</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr></thead></table><p class="c24 c19"><span class="c12">80570 rows &times; 420 columns</span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">import</span><span class="c4">&nbsp;numpy </span><span class="c3">as</span><span class="c4">&nbsp;np<br></span><span class="c3">import</span><span class="c4">&nbsp;pandas </span><span class="c3">as</span><span class="c4">&nbsp;pd<br></span><span class="c3">import</span><span class="c4">&nbsp;matplotlib.pyplot </span><span class="c3">as</span><span class="c4">&nbsp;plt<br></span><span class="c3">from</span><span class="c4">&nbsp;scipy </span><span class="c3">import</span><span class="c4">&nbsp;stats<br>df</span><span class="c3">=</span><span class="c4">df<br></span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;columns_to_examine :<br><br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Plot original data</span><span class="c4"><br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">figure(figsize</span><span class="c3">=</span><span class="c4">(12, 6))<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">subplot(1, 2, 1)<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">title(&#39;Original Data&#39;)<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">hist(df[col], bins</span><span class="c3">=</span><span class="c4">30, alpha</span><span class="c3">=</span><span class="c4">0.5, label</span><span class="c3">=</span><span class="c4">col</span><span class="c3">+</span><span class="c4">&#39; (original)&#39;)<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">legend()<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Normalize the column</span><span class="c4"><br> &nbsp; &nbsp;skewness </span><span class="c3">=</span><span class="c4">&nbsp;df[col]</span><span class="c3">.</span><span class="c4">skew()<br> &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;abs(skewness) </span><span class="c3">&gt;</span><span class="c4">&nbsp;1:<br> &nbsp; &nbsp; &nbsp; &nbsp;df[col], _ </span><span class="c3">=</span><span class="c4">&nbsp;stats</span><span class="c3">.</span><span class="c4">boxcox(df[col] </span><span class="c3">+</span><span class="c4">&nbsp;1)<br> &nbsp; &nbsp;</span><span class="c3">elif</span><span class="c4">&nbsp;abs(skewness) </span><span class="c3">&gt;</span><span class="c4">&nbsp;0.5:<br> &nbsp; &nbsp; &nbsp; &nbsp;df[col] </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">sqrt(df[col])<br> &nbsp; &nbsp;</span><span class="c3">else</span><span class="c4">:<br> &nbsp; &nbsp; &nbsp; &nbsp;df[col] </span><span class="c3">=</span><span class="c4">&nbsp;(df[col] </span><span class="c3">-</span><span class="c4">&nbsp;df[col]</span><span class="c3">.</span><span class="c4">mean()) </span><span class="c3">/</span><span class="c4">&nbsp;df[col]</span><span class="c3">.</span><span class="c4">std()<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Plot transformed data</span><span class="c4"><br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">subplot(1, 2, 2)<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">title(&#39;Transformed Data&#39;)<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">hist(df[col], bins</span><span class="c3">=</span><span class="c4">30, alpha</span><span class="c3">=</span><span class="c4">0.5, label</span><span class="c3">=</span><span class="c4">&nbsp;col</span><span class="c3">+</span><span class="c4">&#39; (transformed)&#39;)<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">legend()<br> &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c2">show()<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9 c19"><span class="c2"></span></p><h2 class="c21 c19"><span class="c27 c3"></span></h2><p class="c23 c19"><span>Nueral Network</span><span class="c29"><a class="c26" href="#id.3znysh7">&para;</a></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.callbacks </span><span class="c3">import</span><span class="c4">&nbsp;TensorBoard<br>tensorboard </span><span class="c3">=</span><span class="c4">&nbsp;TensorBoard(log_dir</span><span class="c3">=</span><span class="c4">&#39;logs/&#39;, histogram_freq</span><span class="c3">=</span><span class="c2">1)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">df</span><span class="c3">.</span><span class="c2">columns<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14"><span class="c12"></span></p><p class="c11"><span class="c6">Index([&#39;dur&#39;, &#39;proto&#39;, &#39;spkts&#39;, &#39;dpkts&#39;, &#39;sbytes&#39;, &#39;dbytes&#39;, &#39;rate&#39;, &#39;sload&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;dload&#39;, &#39;sloss&#39;,<br> &nbsp; &nbsp; &nbsp; ...<br> &nbsp; &nbsp; &nbsp; &#39;hot_ct_srv_dst_51&#39;, &#39;hot_ct_srv_dst_52&#39;, &#39;hot_attack_cat_Backdoor&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;hot_attack_cat_DoS&#39;, &#39;hot_attack_cat_Exploits&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;hot_attack_cat_Fuzzers&#39;, &#39;hot_attack_cat_Generic&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;hot_attack_cat_Normal&#39;, &#39;hot_attack_cat_Reconnaissance&#39;,<br> &nbsp; &nbsp; &nbsp; &#39;hot_attack_cat_Worms&#39;],<br> &nbsp; &nbsp; &nbsp;dtype=&#39;object&#39;, length=420)</span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">import</span><span class="c4">&nbsp;matplotlib.pyplot </span><span class="c3">as</span><span class="c4">&nbsp;plt<br></span><span class="c3">import</span><span class="c4">&nbsp;numpy </span><span class="c3">as</span><span class="c4">&nbsp;np<br></span><span class="c3">from</span><span class="c4">&nbsp;scipy </span><span class="c3">import</span><span class="c4">&nbsp;stats<br></span><span class="c3">import</span><span class="c4">&nbsp;pandas </span><span class="c3">as</span><span class="c4">&nbsp;pd<br><br></span><span class="c3">def</span><span class="c4">&nbsp;plot_and_transform(df, columns_to_examine):<br> &nbsp; &nbsp;new_df </span><span class="c3">=</span><span class="c4">&nbsp;df &nbsp;</span><span class="c4 c13"># Create a copy of the original DataFrame</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;columns_to_examine:<br><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Plot original data</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">figure(figsize</span><span class="c3">=</span><span class="c4">(12, 6))<br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">subplot(1, 2, 1)<br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">title(&#39;Original Data&#39;)<br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">hist(df[col], bins</span><span class="c3">=</span><span class="c4">30, alpha</span><span class="c3">=</span><span class="c4">0.5, label</span><span class="c3">=</span><span class="c4">col </span><span class="c3">+</span><span class="c4">&nbsp;&#39; (original)&#39;)<br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">legend()<br><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Normalize the column</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;skewness </span><span class="c3">=</span><span class="c4">&nbsp;new_df[col]</span><span class="c3">.</span><span class="c4">skew()<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;abs(skewness) </span><span class="c3">&gt;</span><span class="c4">&nbsp;1:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new_df[col], _ </span><span class="c3">=</span><span class="c4">&nbsp;stats</span><span class="c3">.</span><span class="c4">boxcox(new_df[col] </span><span class="c3">+</span><span class="c4">&nbsp;1)<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c3">elif</span><span class="c4">&nbsp;abs(skewness) </span><span class="c3">&gt;</span><span class="c4">&nbsp;0.5:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new_df[col] </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">sqrt(new_df[col])<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c3">else</span><span class="c4">:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;new_df[col] </span><span class="c3">=</span><span class="c4">&nbsp;(new_df[col] </span><span class="c3">-</span><span class="c4">&nbsp;new_df[col]</span><span class="c3">.</span><span class="c4">mean()) </span><span class="c3">/</span><span class="c4">&nbsp;new_df[col]</span><span class="c3">.</span><span class="c4">std()<br><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Plot transformed data</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">subplot(1, 2, 2)<br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">title(&#39;Transformed Data&#39;)<br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">hist(new_df[col], bins</span><span class="c3">=</span><span class="c4">30, alpha</span><span class="c3">=</span><span class="c4">0.5, label</span><span class="c3">=</span><span class="c4">col </span><span class="c3">+</span><span class="c4">&nbsp;&#39; (transformed)&#39;)<br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">legend()<br> &nbsp; &nbsp; &nbsp; &nbsp;plt</span><span class="c3">.</span><span class="c4">show()<br><br> &nbsp; &nbsp;</span><span class="c3">return</span><span class="c4">&nbsp;new_df &nbsp;</span><span class="c4 c13"># Return the modified DataFrame</span><span class="c4"><br><br></span><span class="c4 c13"># Example usage:</span><span class="c4"><br></span><span class="c4 c13"># new_df = plot_and_transform(df, [&#39;column1&#39;, &#39;column2&#39;])</span><span class="c2"><br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">df</span><span class="c3">.</span><span class="c2">tail()<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14 c19"><span class="c12"></span></p><p class="c14 c19"><span class="c12"></span></p><a id="t.ef84f9ee6f5f29368e43e17543f8df362861cc99"></a><a id="t.3"></a><table class="c7"><thead><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c25"><span class="c12"></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dur</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">proto</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">spkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dpkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">rate</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sloss</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_ct_srv_dst_51</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_ct_srv_dst_52</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Backdoor</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_DoS</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Exploits</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Fuzzers</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Generic</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Normal</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Reconnaissance</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Worms</span></p><p class="c22 c19"><span class="c1">&nbsp;</span></p></td><tbody></tbody></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80565</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000006</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">18.164815</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">36.394263</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80566</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">17.295820</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">34.983254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80567</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">17.295820</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">34.983254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80568</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">17.295820</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">34.983254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80569</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">114</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">17.295820</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">34.983254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr></thead></table><p class="c24 c19"><span class="c12">5 rows &times; 420 columns</span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">extraCleansed_test[&#39;proto&#39;] </span><span class="c3">=</span><span class="c4">&nbsp;extraCleansed_test[&#39;proto&#39;]</span><span class="c3">.</span><span class="c4">map({&#39;tcp&#39;: 1, &#39;udp&#39;: 0})<br><br>one_hot_columns,test_tf</span><span class="c3">=</span><span class="c4">&nbsp;hot_encode_columns(extraCleansed_test,threshold</span><span class="c3">=</span><span class="c4">260)<br>test_tf</span><span class="c3">=</span><span class="c2">&nbsp;plot_and_transform(test_tf, columns_to_examine)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">test_tf</span><span class="c3">.</span><span class="c2">tail()<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14 c19"><span class="c12"></span></p><p class="c14 c19"><span class="c12"></span></p><a id="t.24b35ca0e00ba211f2b03f9178adb95f3cabd726"></a><a id="t.4"></a><table class="c7"><thead><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c25"><span class="c12"></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dur</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">proto</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">spkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dpkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">rate</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sinpkt</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_ct_srv_dst_58</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_ct_srv_dst_59</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Backdoor</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_DoS</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Exploits</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Fuzzers</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Generic</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Normal</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Reconnaissance</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">hot_attack_cat_Worms</span></p><p class="c19 c22"><span class="c1">&nbsp;</span></p></td><tbody></tbody></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35150</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.293586</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68199</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">612</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.519505</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">26.181752</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6.791989</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.567635</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35151</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.294315</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68199</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">612</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.507201</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">26.153392</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6.786891</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.578599</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35152</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.299281</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68199</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">612</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.421585</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">25.956006</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6.751236</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.590797</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35153</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.284564</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">60</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68199</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">612</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5.667295</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">26.522280</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6.852705</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.543758</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">35154</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.344679</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">62</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">20</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">68109</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1026</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.468074</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">23.420093</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6.602458</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.775936</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td></tr></thead></table><p class="c24 c19"><span class="c12">5 rows &times; 861 columns</span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">test_tf</span><span class="c3">.</span><span class="c4">shape<br><br><br></span><span class="c4 c13"># Find columns that are not shared</span><span class="c4"><br>columns_df1 </span><span class="c3">=</span><span class="c4">&nbsp;set(test_tf</span><span class="c3">.</span><span class="c4">columns)<br>columns_df2 </span><span class="c3">=</span><span class="c4">&nbsp;set(df</span><span class="c3">.</span><span class="c4">columns)<br><br>not_shared_columns </span><span class="c3">=</span><span class="c4">&nbsp;columns_df1</span><span class="c3">.</span><span class="c4">symmetric_difference(columns_df2)<br>print(&quot;Columns not shared between the two DataFrames:&quot;, not_shared_columns)<br>total_Tr</span><span class="c3">=</span><span class="c4">0<br>total_Te</span><span class="c3">=</span><span class="c4">0<br></span><span class="c3">for</span><span class="c4">&nbsp;column </span><span class="c3">in</span><span class="c4">&nbsp;not_shared_columns:<br> &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;column </span><span class="c3">in</span><span class="c4">&nbsp;columns_df1:<br> &nbsp; &nbsp; &nbsp; &nbsp;total_Te</span><span class="c3">=</span><span class="c4">total_Te</span><span class="c3">+</span><span class="c4">1<br> &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;column </span><span class="c3">in</span><span class="c4">&nbsp;columns_df2:<br> &nbsp; &nbsp; &nbsp; total_Tr</span><span class="c3">=</span><span class="c4">total_Tr</span><span class="c3">+</span><span class="c4">1<br>print(&#39;total&#39;</span><span class="c3">+</span><span class="c4">&nbsp;str(total_Te))<br>print(&#39;total&#39;</span><span class="c3">+</span><span class="c4">&nbsp;str(total_Tr))<br><br></span><span class="c3">if</span><span class="c4">&nbsp;total_Te </span><span class="c3">&gt;</span><span class="c4">&nbsp;total_Tr:<br> &nbsp; &nbsp;test_tf</span><span class="c3">.</span><span class="c4">drop(columns</span><span class="c3">=</span><span class="c4">[col </span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;not_shared_columns </span><span class="c3">if</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;columns_df1], inplace</span><span class="c3">=True</span><span class="c4">)<br> &nbsp; &nbsp;</span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;[col </span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;not_shared_columns </span><span class="c3">if</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;columns_df2]:<br> &nbsp; &nbsp; &nbsp; &nbsp;test_tf[col] </span><span class="c3">=</span><span class="c4">&nbsp;0 &nbsp;</span><span class="c4 c13"># initialize with zeros</span><span class="c4"><br>test_tf </span><span class="c3">=</span><span class="c4">&nbsp;test_tf</span><span class="c3">.</span><span class="c4">reindex(sorted(test_tf</span><span class="c3">.</span><span class="c4">columns), axis</span><span class="c3">=</span><span class="c4">1)<br>df</span><span class="c3">=</span><span class="c4">df</span><span class="c3">.</span><span class="c4">reindex(sorted(df</span><span class="c3">.</span><span class="c4">columns), axis</span><span class="c3">=</span><span class="c2">1)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">Columns not shared between the two DataFrames: set()<br>total0<br>total0<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">test_tf</span><span class="c3">.</span><span class="c4">tail()<br>df</span><span class="c3">.</span><span class="c2">head()<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14 c19"><span class="c12"></span></p><p class="c14 c19"><span class="c12"></span></p><a id="t.9d7cf27ddca04a1b32744f07f8b0b7758972101b"></a><a id="t.5"></a><table class="c7"><thead><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c25"><span class="c12"></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ackdat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dinpkt</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">djit</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dloss</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dmean</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dpkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dtcpb</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dur</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sinpkt</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sjit</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sloss</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">smean</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">spkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">stcpb</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">swin</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">synack</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">tcprtt</span></p><p class="c22 c19"><span class="c1">&nbsp;</span></p></td><tbody></tbody></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">770</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.076256</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3.232083</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6.951521</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">64</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.724011</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.387386</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.187251</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3.686138</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10.554484</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">52</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.739236</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.021395</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2212</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.045820</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.600905</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">7.584815</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">79</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">28</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.656690</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.399717</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.913582</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.542055</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">19.352890</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">28</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">909</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">62</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.906903</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.018450</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.039412</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.020613</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1096</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.960645</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.491322</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8.186304</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">137</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.082292</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.231460</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.968896</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.420881</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.775308</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">86</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.152212</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.018843</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.038855</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">3</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.021823</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">268</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.994504</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.529934</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">7.280236</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">45</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.124919</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.211707</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.919755</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.392143</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14.372310</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">100</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.031262</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.018604</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.040243</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">4</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.020529</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">950</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.004668</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.465016</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">7.499679</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">79</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">11.667646</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.341113</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.073796</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.589100</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">11.729767</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">49</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.703499</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.018431</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.038355</span></p></td></tr></thead></table><p class="c24 c19"><span class="c12">5 rows &times; 420 columns</span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">attack_cat_cols </span><span class="c3">=</span><span class="c4">&nbsp;[col </span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;test_tf</span><span class="c3">.</span><span class="c4">columns </span><span class="c3">if</span><span class="c4">&nbsp;&#39;hot_attack_cat_&#39; </span><span class="c3">in</span><span class="c2">&nbsp;col]<br>print(attack_cat_cols)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">[&#39;hot_attack_cat_Backdoor&#39;, &#39;hot_attack_cat_DoS&#39;, &#39;hot_attack_cat_Exploits&#39;, &#39;hot_attack_cat_Fuzzers&#39;, &#39;hot_attack_cat_Generic&#39;, &#39;hot_attack_cat_Normal&#39;, &#39;hot_attack_cat_Reconnaissance&#39;, &#39;hot_attack_cat_Worms&#39;]<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">import</span><span class="c4">&nbsp;pandas </span><span class="c3">as</span><span class="c4">&nbsp;pd<br><br>data</span><span class="c3">=</span><span class="c4">df<br>data_2</span><span class="c3">=</span><span class="c4">test_tf<br><br><br><br></span><span class="c4 c13"># Assuming `data` is your DataFrame</span><span class="c4"><br></span><span class="c4 c13"># Extract label and one-hot-encoded attack categories</span><span class="c4"><br>label_col </span><span class="c3">=</span><span class="c4">&nbsp;data[&#39;label&#39;]<br>attack_cat_cols </span><span class="c3">=</span><span class="c4">&nbsp;[col </span><span class="c3">for</span><span class="c4">&nbsp;col </span><span class="c3">in</span><span class="c4">&nbsp;data</span><span class="c3">.</span><span class="c4">columns </span><span class="c3">if</span><span class="c4">&nbsp;&#39;hot_attack_cat_&#39; </span><span class="c3">in</span><span class="c4">&nbsp;col]<br><br></span><span class="c4 c13"># Remove these columns from the DataFrame to get feature columns</span><span class="c4"><br><br>Y_attack_cat_train </span><span class="c3">=</span><span class="c4">&nbsp;data[attack_cat_cols]<br>Y_attack_cat_test </span><span class="c3">=</span><span class="c4">&nbsp;data_2[attack_cat_cols]<br><br><br>Y_label_train </span><span class="c3">=</span><span class="c4">&nbsp;label_col<br>Y_label_test </span><span class="c3">=</span><span class="c4">&nbsp;data_2[&#39;label&#39;]<br></span><span class="c4 c13">#Y_label_test=pd.Series(np.zeros(len(Y_label_test)), index=Y_label_test.index)</span><span class="c4"><br><br>X_train </span><span class="c3">=</span><span class="c4">&nbsp;data</span><span class="c3">.</span><span class="c4">drop([&#39;label&#39;] </span><span class="c3">+</span><span class="c4">&nbsp;attack_cat_cols, axis</span><span class="c3">=</span><span class="c4">1)<br>X_test</span><span class="c3">=</span><span class="c4">&nbsp;data_2</span><span class="c3">.</span><span class="c4">drop([&#39;label&#39;] </span><span class="c3">+</span><span class="c4">&nbsp;attack_cat_cols, axis</span><span class="c3">=</span><span class="c2">1)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9 c19"><span class="c2"></span></p><h2 class="c21 c19"><span class="c27 c3"></span></h2><p class="c23 c19"><span>SVM</span><span class="c29"><a class="c26" href="#id.2et92p0">&para;</a></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras </span><span class="c3">import</span><span class="c4">&nbsp;backend </span><span class="c3">as</span><span class="c4">&nbsp;K<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.layers </span><span class="c3">import</span><span class="c4">&nbsp;Input, Dense, Conv1D, Flatten, Concatenate, Reshape<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.models </span><span class="c3">import</span><span class="c4">&nbsp;Model<br></span><span class="c3">import</span><span class="c4">&nbsp;tensorflow </span><span class="c3">as</span><span class="c4">&nbsp;tf<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras </span><span class="c3">import</span><span class="c4">&nbsp;metrics<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.callbacks </span><span class="c3">import</span><span class="c4">&nbsp;EarlyStopping<br>precision_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Precision()<br>recall_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Recall()<br>early_stopping </span><span class="c3">=</span><span class="c4">&nbsp;EarlyStopping(<br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;val_loss&#39;, &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Monitor validation loss</span><span class="c4"><br> &nbsp; &nbsp;patience</span><span class="c3">=</span><span class="c4">3, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Number of epochs with no improvement to wait</span><span class="c4"><br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c4 c13"># Print additional logs</span><span class="c4"><br> &nbsp; &nbsp;restore_best_weights</span><span class="c3">=True</span><span class="c4">&nbsp;</span><span class="c4 c13"># Restore the best weights when stopped</span><span class="c4"><br>)<br>model_checkpoint </span><span class="c3">=</span><span class="c4">&nbsp;ModelCheckpoint(<br> &nbsp; &nbsp;filepath</span><span class="c3">=</span><span class="c4">&#39;best_1_model.h5&#39;, &nbsp;</span><span class="c4 c13"># Replace with your desired file path and name</span><span class="c4"><br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;val_loss&#39;,<br> &nbsp; &nbsp;save_best_only</span><span class="c3">=True</span><span class="c4">,<br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1<br>)<br></span><span class="c4 c13"># Compile model</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">compile(<br> &nbsp; &nbsp;optimizer</span><span class="c3">=</span><span class="c4">&#39;adam&#39;,<br> &nbsp; &nbsp;loss</span><span class="c3">=</span><span class="c4">&#39;binary_crossentropy&#39;,<br> &nbsp; &nbsp;metrics</span><span class="c3">=</span><span class="c4">[<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">BinaryAccuracy(name</span><span class="c3">=</span><span class="c4">&#39;accuracy&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Precision(name</span><span class="c3">=</span><span class="c4">&#39;precision&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Recall(name</span><span class="c3">=</span><span class="c4">&#39;recall&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">AUC(name</span><span class="c3">=</span><span class="c4">&#39;auc&#39;)<br> &nbsp; &nbsp;]<br>)<br><br><br></span><span class="c4 c13"># Define the input layer</span><span class="c4"><br>input_dim </span><span class="c3">=</span><span class="c4">&nbsp;411 &nbsp;</span><span class="c4 c13"># Assume input_dim = 411</span><span class="c4"><br>input_layer </span><span class="c3">=</span><span class="c4">&nbsp;Input(shape</span><span class="c3">=</span><span class="c4">(input_dim,))<br><br></span><span class="c4 c13"># Define dense layers</span><span class="c4"><br>hidden_layer_1 </span><span class="c3">=</span><span class="c4">&nbsp;Dense(128, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(input_layer)<br>hidden_layer_2 </span><span class="c3">=</span><span class="c4">&nbsp;Dense(64, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(hidden_layer_1)<br>hidden_layer_3 </span><span class="c3">=</span><span class="c4">&nbsp;Dense(32, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(hidden_layer_2)<br><br></span><span class="c4 c13"># 1D Conv layer</span><span class="c4"><br>conv_input_reshaped </span><span class="c3">=</span><span class="c4">&nbsp;Reshape((input_dim, 1))(input_layer)<br>conv_layer </span><span class="c3">=</span><span class="c4">&nbsp;Conv1D(filters</span><span class="c3">=</span><span class="c4">32, kernel_size</span><span class="c3">=</span><span class="c4">3, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(conv_input_reshaped)<br>conv_layer </span><span class="c3">=</span><span class="c4">&nbsp;Flatten()(conv_layer)<br><br></span><span class="c4 c13"># Concatenate dense and conv layers</span><span class="c4"><br>concat_layer </span><span class="c3">=</span><span class="c4">&nbsp;Concatenate()([hidden_layer_3, conv_layer])<br><br></span><span class="c4 c13"># SVM layer for binary classification</span><span class="c4"><br>output_layer </span><span class="c3">=</span><span class="c4">&nbsp;Dense(1, activation</span><span class="c3">=None</span><span class="c4">, kernel_regularizer</span><span class="c3">=</span><span class="c4">tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">regularizers</span><span class="c3">.</span><span class="c4">l2())(concat_layer) &nbsp;</span><span class="c4 c13"># No activation function</span><span class="c4"><br><br></span><span class="c4 c13"># Build the model</span><span class="c4"><br>model </span><span class="c3">=</span><span class="c4">&nbsp;Model(inputs</span><span class="c3">=</span><span class="c4">input_layer, outputs</span><span class="c3">=</span><span class="c4">output_layer)<br><br></span><span class="c4 c13"># Compile the model using hinge loss</span><span class="c4"><br>model_checkpoint </span><span class="c3">=</span><span class="c4">&nbsp;ModelCheckpoint(<br> &nbsp; &nbsp;filepath</span><span class="c3">=</span><span class="c4">&#39;best_1_model.h5&#39;, &nbsp;</span><span class="c4 c13"># Replace with your desired file path and name</span><span class="c4"><br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;val_loss&#39;,<br> &nbsp; &nbsp;save_best_only</span><span class="c3">=True</span><span class="c4">,<br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1<br>)<br></span><span class="c4 c13"># Compile model</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">compile(<br> &nbsp; &nbsp;optimizer</span><span class="c3">=</span><span class="c4">&#39;adam&#39;,<br> &nbsp; &nbsp;loss</span><span class="c3">=</span><span class="c4">&#39;binary_crossentropy&#39;,<br> &nbsp; &nbsp;metrics</span><span class="c3">=</span><span class="c4">[<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">BinaryAccuracy(name</span><span class="c3">=</span><span class="c4">&#39;accuracy&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Precision(name</span><span class="c3">=</span><span class="c4">&#39;precision&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Recall(name</span><span class="c3">=</span><span class="c4">&#39;recall&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">AUC(name</span><span class="c3">=</span><span class="c4">&#39;auc&#39;)<br> &nbsp; &nbsp;]<br>)<br><br>history </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">fit(<br> &nbsp; &nbsp;X_train,<br> &nbsp; &nbsp;Y_label_train,<br> &nbsp; &nbsp;epochs</span><span class="c3">=</span><span class="c4">10,<br> &nbsp; &nbsp;batch_size</span><span class="c3">=</span><span class="c4">32,<br> &nbsp; &nbsp;validation_split</span><span class="c3">=</span><span class="c4">0.2,<br> &nbsp; &nbsp;callbacks</span><span class="c3">=</span><span class="c4">[tensorboard,early_stopping,model_checkpoint]<br>)<br><br></span><span class="c4 c13"># Evaluate the model</span><span class="c4"><br>loss, accuracy, precision, recall, auc </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">evaluate(X_test, Y_label_test)<br></span><span class="c3">if</span><span class="c4">&nbsp;precision </span><span class="c3">+</span><span class="c4">&nbsp;recall </span><span class="c3">==</span><span class="c4">&nbsp;0:<br> &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;0<br></span><span class="c3">else</span><span class="c4">:<br> &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;2 </span><span class="c3">*</span><span class="c4">&nbsp;(precision </span><span class="c3">*</span><span class="c4">&nbsp;recall) </span><span class="c3">/</span><span class="c4">&nbsp;(precision </span><span class="c3">+</span><span class="c4">&nbsp;recall)<br>print(f&quot;Accuracy: {accuracy}&quot;)<br>print(f&quot;Precision: {precision}&quot;)<br>print(f&quot;Recall: {recall}&quot;)<br>print(f&quot;F1 Score: {f1}&quot;)<br>print(f&quot;AUC: {auc}&quot;)<br><br><br><br></span><span class="c3">from</span><span class="c4">&nbsp;sklearn.metrics </span><span class="c3">import</span><span class="c4">&nbsp;recall_score, precision_score, f1_score, confusion_matrix<br><br></span><span class="c4 c13"># Prediction and Thresholding</span><span class="c4"><br>predictions </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">predict(X_test)<br>threshold </span><span class="c3">=</span><span class="c4">&nbsp;0.5<br>predicted_labels </span><span class="c3">=</span><span class="c4">&nbsp;(predictions </span><span class="c3">&gt;</span><span class="c4">&nbsp;threshold)</span><span class="c3">.</span><span class="c4">astype(int)<br><br></span><span class="c4 c13"># Ensure True Labels are in Correct Format</span><span class="c4"><br>true_labels </span><span class="c3">=</span><span class="c4">&nbsp;Y_label_test</span><span class="c3">.</span><span class="c4">to_numpy()<br><br></span><span class="c4 c13"># Compute Metrics</span><span class="c4"><br></span><span class="c4 c13"># Recall</span><span class="c4"><br>recall </span><span class="c3">=</span><span class="c4">&nbsp;recall_score(true_labels, predicted_labels)<br>print(f&quot;Recall: {recall}&quot;)<br><br></span><span class="c4 c13"># Precision</span><span class="c4"><br>precision </span><span class="c3">=</span><span class="c4">&nbsp;precision_score(true_labels, predicted_labels)<br>print(f&quot;Precision: {precision}&quot;)<br><br></span><span class="c4 c13"># F1 Score</span><span class="c4"><br>f1 </span><span class="c3">=</span><span class="c4">&nbsp;f1_score(true_labels, predicted_labels)<br>print(f&quot;F1 Score: {f1}&quot;)<br><br></span><span class="c4 c13"># Confusion Matrix</span><span class="c4"><br>conf_matrix </span><span class="c3">=</span><span class="c4">&nbsp;confusion_matrix(true_labels, predicted_labels)<br>print(f&quot;Confusion Matrix: \n{conf_matrix}&quot;)<br></span><span class="c3">from</span><span class="c4">&nbsp;sklearn.metrics </span><span class="c3">import</span><span class="c4">&nbsp;accuracy_score<br>accuracy </span><span class="c3">=</span><span class="c2">&nbsp;accuracy_score(true_labels, predicted_labels)<br>print(&quot;Test Accuracy:&quot;, accuracy)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">Epoch 1/10<br>2012/2015 [============================&gt;.] - ETA: 0s - loss: 10.7597 - accuracy: 0.3025 - precision: 0.5783 - recall: 0.0011 - auc: 0.4996<br>Epoch 1: val_loss improved from inf to 15.42299, saving model to best_1_model.h5<br>2015/2015 [==============================] - 24s 11ms/step - loss: 10.7618 - accuracy: 0.3023 - precision: 0.5783 - recall: 0.0011 - auc: 0.4996 - val_loss: 15.4230 - val_accuracy: 1.2412e-04 - val_precision: 1.0000 - val_recall: 1.2412e-04 - val_auc: 0.0000e+00<br>Epoch 2/10<br> &nbsp;14/2015 [..............................] - ETA: 16s - loss: 10.6391 - accuracy: 0.3103 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000</span></p><p class="c9"><span class="c6"></span></p><p class="c14"><span class="c6"></span></p><p class="c11"><span class="c6">/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save(&#39;my_model.keras&#39;)`.<br> &nbsp;saving_api.save_model(<br></span></p><p class="c9"><span class="c6"></span></p><p class="c14"><span class="c6"></span></p><p class="c11"><span class="c6">2012/2015 [============================&gt;.] - ETA: 0s - loss: 10.7626 - accuracy: 0.3023 - precision: 0.8462 - recall: 2.4481e-04 - auc: 0.5001<br>Epoch 2: val_loss did not improve from 15.42299<br>2015/2015 [==============================] - 21s 10ms/step - loss: 10.7627 - accuracy: 0.3023 - precision: 0.8462 - recall: 2.4454e-04 - auc: 0.5001 - val_loss: 15.4249 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00<br>Epoch 3/10<br>2012/2015 [============================&gt;.] - ETA: 0s - loss: 10.7657 - accuracy: 0.3021 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000<br>Epoch 3: val_loss did not improve from 15.42299<br>2015/2015 [==============================] - 22s 11ms/step - loss: 10.7649 - accuracy: 0.3021 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000 - val_loss: 15.4249 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00<br>Epoch 4/10<br>2015/2015 [==============================] - ETA: 0s - loss: 10.7649 - accuracy: 0.3021 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000Restoring model weights from the end of the best epoch: 1.<br><br>Epoch 4: val_loss did not improve from 15.42299<br>2015/2015 [==============================] - 21s 10ms/step - loss: 10.7649 - accuracy: 0.3021 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.5000 - val_loss: 15.4249 - val_accuracy: 0.0000e+00 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_auc: 0.0000e+00<br>Epoch 4: early stopping<br>1099/1099 [==============================] - 5s 4ms/step - loss: 11.2074 - accuracy: 0.2734 - precision: 0.8462 - recall: 4.3049e-04 - auc: 0.5001<br>Accuracy: 0.27341771125793457<br>Precision: 0.8461538553237915<br>Recall: 0.0004304946633055806<br>F1 Score: 0.0008605515069062791<br>AUC: 0.5001111030578613<br>1099/1099 [==============================] - 5s 5ms/step<br>Recall: 0.00043049467752035066<br>Precision: 0.8461538461538461<br>F1 Score: 0.000860551535302171<br>Confusion Matrix: <br>[[ 9601 &nbsp; &nbsp; 2]<br> [25541 &nbsp; &nbsp;11]]<br>Test Accuracy: 0.27341772151898736<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9 c19"><span class="c6"></span></p><h2 class="c21 c19"><span class="c27 c28"></span></h2><p class="c23 c19"><span>Model with a CNN</span><span class="c29"><a class="c26" href="#id.tyjcwt">&para;</a></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">X_train</span><span class="c3">.</span><span class="c2">shape[1]<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14"><span class="c12"></span></p><p class="c11"><span class="c6">411</span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras </span><span class="c3">import</span><span class="c4">&nbsp;Input, Model<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.layers </span><span class="c3">import</span><span class="c4">&nbsp;Dense, Conv1D, Flatten, concatenate<br><br>input_layer </span><span class="c3">=</span><span class="c4">&nbsp;Input(shape</span><span class="c3">=</span><span class="c4">(X_train</span><span class="c3">.</span><span class="c4">shape[1],))<br><br><br></span><span class="c4 c13"># Define the input layer</span><span class="c4"><br>input_layer </span><span class="c3">=</span><span class="c4">&nbsp;Input(shape</span><span class="c3">=</span><span class="c4">(input_dim,))<br><br></span><span class="c4 c13"># Define dense layers</span><span class="c4"><br>hidden_layer_1 </span><span class="c3">=</span><span class="c4">&nbsp;Dense(128, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(input_layer)<br>hidden_layer_2 </span><span class="c3">=</span><span class="c4">&nbsp;Dense(64, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(hidden_layer_1)<br>hidden_layer_3 </span><span class="c3">=</span><span class="c4">&nbsp;Dense(32, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(hidden_layer_2)<br><br></span><span class="c4 c13"># 1D Conv layer</span><span class="c4"><br>conv_input_reshaped </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Reshape((input_dim, 1))(input_layer)<br>conv_layer </span><span class="c3">=</span><span class="c4">&nbsp;Conv1D(filters</span><span class="c3">=</span><span class="c4">32, kernel_size</span><span class="c3">=</span><span class="c4">3, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(conv_input_reshaped)<br>conv_layer </span><span class="c3">=</span><span class="c4">&nbsp;Flatten()(conv_layer)<br><br></span><span class="c4 c13"># Concatenate dense and conv layers</span><span class="c4"><br>concat_layer </span><span class="c3">=</span><span class="c4">&nbsp;Concatenate()([hidden_layer_3, conv_layer])<br><br></span><span class="c4 c13"># Output layer for binary classification</span><span class="c4"><br>output_layer </span><span class="c3">=</span><span class="c4">&nbsp;Dense(1, activation</span><span class="c3">=</span><span class="c4">&#39;sigmoid&#39;)(concat_layer)<br><br></span><span class="c4 c13"># Build the model</span><span class="c4"><br>model </span><span class="c3">=</span><span class="c4">&nbsp;Model(inputs</span><span class="c3">=</span><span class="c4">input_layer, outputs</span><span class="c3">=</span><span class="c4">output_layer)<br><br><br><br><br><br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras </span><span class="c3">import</span><span class="c4">&nbsp;metrics<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.callbacks </span><span class="c3">import</span><span class="c4">&nbsp;EarlyStopping<br>precision_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Precision()<br>recall_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Recall()<br>early_stopping </span><span class="c3">=</span><span class="c4">&nbsp;EarlyStopping(<br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;val_loss&#39;, &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Monitor validation loss</span><span class="c4"><br> &nbsp; &nbsp;patience</span><span class="c3">=</span><span class="c4">3, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Number of epochs with no improvement to wait</span><span class="c4"><br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c4 c13"># Print additional logs</span><span class="c4"><br> &nbsp; &nbsp;restore_best_weights</span><span class="c3">=True</span><span class="c4">&nbsp;</span><span class="c4 c13"># Restore the best weights when stopped</span><span class="c4"><br>)<br>model_checkpoint </span><span class="c3">=</span><span class="c4">&nbsp;ModelCheckpoint(<br> &nbsp; &nbsp;filepath</span><span class="c3">=</span><span class="c4">&#39;best_1_model.h5&#39;, &nbsp;</span><span class="c4 c13"># Replace with your desired file path and name</span><span class="c4"><br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;val_loss&#39;,<br> &nbsp; &nbsp;save_best_only</span><span class="c3">=True</span><span class="c4">,<br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1<br>)<br></span><span class="c4 c13"># Compile model</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">compile(<br> &nbsp; &nbsp;optimizer</span><span class="c3">=</span><span class="c4">&#39;adam&#39;,<br> &nbsp; &nbsp;loss</span><span class="c3">=</span><span class="c4">&#39;binary_crossentropy&#39;,<br> &nbsp; &nbsp;metrics</span><span class="c3">=</span><span class="c4">[<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">BinaryAccuracy(name</span><span class="c3">=</span><span class="c4">&#39;accuracy&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Precision(name</span><span class="c3">=</span><span class="c4">&#39;precision&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Recall(name</span><span class="c3">=</span><span class="c4">&#39;recall&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">AUC(name</span><span class="c3">=</span><span class="c4">&#39;auc&#39;)<br> &nbsp; &nbsp;]<br>)<br><br></span><span class="c4 c13"># Train the model</span><span class="c4"><br><br>history </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">fit(<br> &nbsp; &nbsp;X_train,<br> &nbsp; &nbsp;Y_label_train,<br> &nbsp; &nbsp;epochs</span><span class="c3">=</span><span class="c4">10,<br> &nbsp; &nbsp;batch_size</span><span class="c3">=</span><span class="c4">32,<br> &nbsp; &nbsp;validation_split</span><span class="c3">=</span><span class="c4">0.2,<br> &nbsp; &nbsp;callbacks</span><span class="c3">=</span><span class="c4">[tensorboard,early_stopping,model_checkpoint]<br>)<br><br></span><span class="c4 c13"># Evaluate the model</span><span class="c4"><br>loss, accuracy, precision, recall, auc </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">evaluate(X_test, Y_label_test)<br></span><span class="c3">if</span><span class="c4">&nbsp;precision </span><span class="c3">+</span><span class="c4">&nbsp;recall </span><span class="c3">==</span><span class="c4">&nbsp;0:<br> &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;0<br></span><span class="c3">else</span><span class="c4">:<br> &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;2 </span><span class="c3">*</span><span class="c4">&nbsp;(precision </span><span class="c3">*</span><span class="c4">&nbsp;recall) </span><span class="c3">/</span><span class="c4">&nbsp;(precision </span><span class="c3">+</span><span class="c4">&nbsp;recall)<br>print(f&quot;Accuracy: {accuracy}&quot;)<br>print(f&quot;Precision: {precision}&quot;)<br>print(f&quot;Recall: {recall}&quot;)<br>print(f&quot;F1 Score: {f1}&quot;)<br>print(f&quot;AUC: {auc}&quot;)<br><br><br><br></span><span class="c3">from</span><span class="c4">&nbsp;sklearn.metrics </span><span class="c3">import</span><span class="c4">&nbsp;recall_score, precision_score, f1_score, confusion_matrix<br><br></span><span class="c4 c13"># Prediction and Thresholding</span><span class="c4"><br>predictions </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">predict(X_test)<br>threshold </span><span class="c3">=</span><span class="c4">&nbsp;0.5<br>predicted_labels </span><span class="c3">=</span><span class="c4">&nbsp;(predictions </span><span class="c3">&gt;</span><span class="c4">&nbsp;threshold)</span><span class="c3">.</span><span class="c4">astype(int)<br><br></span><span class="c4 c13"># Ensure True Labels are in Correct Format</span><span class="c4"><br>true_labels </span><span class="c3">=</span><span class="c4">&nbsp;Y_label_test</span><span class="c3">.</span><span class="c4">to_numpy()<br><br></span><span class="c4 c13"># Compute Metrics</span><span class="c4"><br></span><span class="c4 c13"># Recall</span><span class="c4"><br>recall </span><span class="c3">=</span><span class="c4">&nbsp;recall_score(true_labels, predicted_labels)<br>print(f&quot;Recall: {recall}&quot;)<br><br></span><span class="c4 c13"># Precision</span><span class="c4"><br>precision </span><span class="c3">=</span><span class="c4">&nbsp;precision_score(true_labels, predicted_labels)<br>print(f&quot;Precision: {precision}&quot;)<br><br></span><span class="c4 c13"># F1 Score</span><span class="c4"><br>f1 </span><span class="c3">=</span><span class="c4">&nbsp;f1_score(true_labels, predicted_labels)<br>print(f&quot;F1 Score: {f1}&quot;)<br><br></span><span class="c4 c13"># Confusion Matrix</span><span class="c4"><br>conf_matrix </span><span class="c3">=</span><span class="c4">&nbsp;confusion_matrix(true_labels, predicted_labels)<br>print(f&quot;Confusion Matrix: \n{conf_matrix}&quot;)<br></span><span class="c3">from</span><span class="c4">&nbsp;sklearn.metrics </span><span class="c3">import</span><span class="c4">&nbsp;accuracy_score<br>accuracy </span><span class="c3">=</span><span class="c2">&nbsp;accuracy_score(true_labels, predicted_labels)<br>print(&quot;Test Accuracy:&quot;, accuracy)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">Epoch 1/10<br>2015/2015 [==============================] - ETA: 0s - loss: 63.2830 - accuracy: 0.8446 - precision: 0.8900 - recall: 0.8869 - auc: 0.8309<br>Epoch 1: val_loss improved from inf to 9.79023, saving model to best_1_model.h5<br>2015/2015 [==============================] - 25s 11ms/step - loss: 63.2830 - accuracy: 0.8446 - precision: 0.8900 - recall: 0.8869 - auc: 0.8309 - val_loss: 9.7902 - val_accuracy: 0.9927 - val_precision: 1.0000 - val_recall: 0.9927 - val_auc: 0.0000e+00<br>Epoch 2/10<br> &nbsp;20/2015 [..............................] - ETA: 16s - loss: 5.6431 - accuracy: 0.9125 - precision: 0.9072 - recall: 0.9729 - auc: 0.8791</span></p><p class="c9"><span class="c6"></span></p><p class="c14"><span class="c6"></span></p><p class="c11"><span class="c6">/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save(&#39;my_model.keras&#39;)`.<br> &nbsp;saving_api.save_model(<br>2010/2015 [============================&gt;.] - ETA: 0s - loss: 23.7671 - accuracy: 0.9417 - precision: 0.9579 - recall: 0.9587 - auc: 0.9411<br>Epoch 2: val_loss improved from 9.79023 to 0.12028, saving model to best_1_model.h5<br>2015/2015 [==============================] - 21s 10ms/step - loss: 23.7196 - accuracy: 0.9417 - precision: 0.9579 - recall: 0.9585 - auc: 0.9411 - val_loss: 0.1203 - val_accuracy: 0.9912 - val_precision: 1.0000 - val_recall: 0.9912 - val_auc: 0.0000e+00<br>Epoch 3/10<br>2009/2015 [============================&gt;.] - ETA: 0s - loss: 2.2408 - accuracy: 0.9675 - precision: 0.9766 - recall: 0.9769 - auc: 0.9712<br>Epoch 3: val_loss improved from 0.12028 to 0.03129, saving model to best_1_model.h5<br>2015/2015 [==============================] - 23s 11ms/step - loss: 2.2372 - accuracy: 0.9675 - precision: 0.9766 - recall: 0.9768 - auc: 0.9712 - val_loss: 0.0313 - val_accuracy: 0.9968 - val_precision: 1.0000 - val_recall: 0.9968 - val_auc: 0.0000e+00<br>Epoch 4/10<br>2012/2015 [============================&gt;.] - ETA: 0s - loss: 2.7337 - accuracy: 0.9685 - precision: 0.9771 - recall: 0.9778 - auc: 0.9715<br>Epoch 4: val_loss did not improve from 0.03129<br>2015/2015 [==============================] - 24s 12ms/step - loss: 2.7308 - accuracy: 0.9685 - precision: 0.9771 - recall: 0.9778 - auc: 0.9715 - val_loss: 0.0491 - val_accuracy: 0.9935 - val_precision: 1.0000 - val_recall: 0.9935 - val_auc: 0.0000e+00<br>Epoch 5/10<br>2010/2015 [============================&gt;.] - ETA: 0s - loss: 0.7675 - accuracy: 0.9736 - precision: 0.9809 - recall: 0.9813 - auc: 0.9799<br>Epoch 5: val_loss improved from 0.03129 to 0.02061, saving model to best_1_model.h5<br>2015/2015 [==============================] - 21s 11ms/step - loss: 0.7660 - accuracy: 0.9737 - precision: 0.9809 - recall: 0.9813 - auc: 0.9799 - val_loss: 0.0206 - val_accuracy: 0.9955 - val_precision: 1.0000 - val_recall: 0.9955 - val_auc: 0.0000e+00<br>Epoch 6/10<br>2009/2015 [============================&gt;.] - ETA: 0s - loss: 0.5342 - accuracy: 0.9728 - precision: 0.9797 - recall: 0.9813 - auc: 0.9819<br>Epoch 6: val_loss did not improve from 0.02061<br>2015/2015 [==============================] - 23s 11ms/step - loss: 0.5328 - accuracy: 0.9728 - precision: 0.9797 - recall: 0.9813 - auc: 0.9819 - val_loss: 0.0280 - val_accuracy: 0.9965 - val_precision: 1.0000 - val_recall: 0.9965 - val_auc: 0.0000e+00<br>Epoch 7/10<br>2010/2015 [============================&gt;.] - ETA: 0s - loss: 0.2118 - accuracy: 0.9760 - precision: 0.9816 - recall: 0.9840 - auc: 0.9883<br>Epoch 7: val_loss improved from 0.02061 to 0.01240, saving model to best_1_model.h5<br>2015/2015 [==============================] - 24s 12ms/step - loss: 0.2116 - accuracy: 0.9759 - precision: 0.9816 - recall: 0.9839 - auc: 0.9883 - val_loss: 0.0124 - val_accuracy: 0.9956 - val_precision: 1.0000 - val_recall: 0.9956 - val_auc: 0.0000e+00<br>Epoch 8/10<br>2013/2015 [============================&gt;.] - ETA: 0s - loss: 0.0965 - accuracy: 0.9771 - precision: 0.9809 - recall: 0.9864 - auc: 0.9922<br>Epoch 8: val_loss did not improve from 0.01240<br>2015/2015 [==============================] - 22s 11ms/step - loss: 0.0965 - accuracy: 0.9770 - precision: 0.9809 - recall: 0.9863 - auc: 0.9922 - val_loss: 0.0194 - val_accuracy: 0.9938 - val_precision: 1.0000 - val_recall: 0.9938 - val_auc: 0.0000e+00<br>Epoch 9/10<br>2009/2015 [============================&gt;.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9784 - precision: 0.9812 - recall: 0.9880 - auc: 0.9947<br>Epoch 9: val_loss improved from 0.01240 to 0.00379, saving model to best_1_model.h5<br>2015/2015 [==============================] - 23s 12ms/step - loss: 0.0757 - accuracy: 0.9784 - precision: 0.9812 - recall: 0.9880 - auc: 0.9947 - val_loss: 0.0038 - val_accuracy: 0.9989 - val_precision: 1.0000 - val_recall: 0.9989 - val_auc: 0.0000e+00<br>Epoch 10/10<br>2014/2015 [============================&gt;.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9805 - precision: 0.9820 - recall: 0.9901 - auc: 0.9959<br>Epoch 10: val_loss did not improve from 0.00379<br>2015/2015 [==============================] - 24s 12ms/step - loss: 0.0816 - accuracy: 0.9805 - precision: 0.9821 - recall: 0.9901 - auc: 0.9959 - val_loss: 0.0101 - val_accuracy: 0.9967 - val_precision: 1.0000 - val_recall: 0.9967 - val_auc: 0.0000e+00<br>1099/1099 [==============================] - 5s 5ms/step - loss: 0.0865 - accuracy: 0.9669 - precision: 0.9644 - recall: 0.9910 - auc: 0.9959<br>Accuracy: 0.966889500617981<br>Precision: 0.964391827583313<br>Recall: 0.991037905216217<br>F1 Score: 0.9775333171880216<br>AUC: 0.9958853125572205<br>1099/1099 [==============================] - 5s 4ms/step<br>Recall: 0.9910378835316218<br>Precision: 0.9643918044024679<br>F1 Score: 0.977533294730747<br>Confusion Matrix: [[ 8668 &nbsp; 935]<br> [ &nbsp;229 25323]]<br>Test Accuracy: 0.9668894894040677<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">%</span><span class="c4">load_ext tensorboard<br></span><span class="c3">%</span><span class="c4">tensorboard </span><span class="c3">--</span><span class="c2">logdir logs<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">The tensorboard extension is already loaded. To reload it, use:<br> &nbsp;%reload_ext tensorboard<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9 c19"><span class="c6"></span></p><h2 class="c19 c21"><span class="c27 c28"></span></h2><p class="c23 c19"><span>FCC</span><span class="c29"><a class="c26" href="#id.3dy6vkm">&para;</a></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">import</span><span class="c4">&nbsp;tensorflow </span><span class="c3">as</span><span class="c4">&nbsp;tf<br><br></span><span class="c4 c13"># Define the model architecture</span><span class="c4"><br>input_dim </span><span class="c3">=</span><span class="c4">&nbsp;X_train</span><span class="c3">.</span><span class="c4">shape[1]<br>output_dim_label </span><span class="c3">=</span><span class="c4">&nbsp;1 &nbsp;</span><span class="c4 c13"># Binary classification</span><span class="c4"><br>output_dim_attack_cat </span><span class="c3">=</span><span class="c4">&nbsp;len(attack_cat_cols) &nbsp;</span><span class="c4 c13"># Number of attack categories</span><span class="c4"><br><br></span><span class="c4 c13"># Define the model</span><span class="c4"><br>model </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">models</span><span class="c3">.</span><span class="c4">Sequential([<br> &nbsp; &nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Dense(128, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;, input_shape</span><span class="c3">=</span><span class="c4">(input_dim,)),<br> &nbsp; &nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Dense(64, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;),<br> &nbsp; &nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Dense(32, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;),<br> &nbsp; &nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Dense(output_dim_label) &nbsp;</span><span class="c4 c13"># No activation here</span><span class="c4"><br>])<br><br></span><span class="c4 c13"># Custom activation split into two</span><span class="c4"><br></span><span class="c3">def</span><span class="c4">&nbsp;custom_activation(x):<br> &nbsp; &nbsp;sigmoid </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">activations</span><span class="c3">.</span><span class="c4">sigmoid(x[:, :output_dim_label])<br> &nbsp; &nbsp;</span><span class="c4 c13">## softmax = tf.keras.activations.softmax(x[:, output_dim_label:])</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c3">return</span><span class="c4">&nbsp;sigmoid </span><span class="c4 c13">## tf.concat([sigmoid, softmax], axis=1)</span><span class="c4"><br><br></span><span class="c4 c13"># Add custom activation layer</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">add(tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c2">Lambda(custom_activation))<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c16"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14"><span class="c12"></span></p><p class="c11"><span class="c6">0 &nbsp; &nbsp; &nbsp; &nbsp;0<br>1 &nbsp; &nbsp; &nbsp; &nbsp;0<br>2 &nbsp; &nbsp; &nbsp; &nbsp;0<br>3 &nbsp; &nbsp; &nbsp; &nbsp;0<br>4 &nbsp; &nbsp; &nbsp; &nbsp;0<br> &nbsp; &nbsp; &nbsp; &nbsp;..<br>35150 &nbsp; &nbsp;0<br>35151 &nbsp; &nbsp;0<br>35152 &nbsp; &nbsp;0<br>35153 &nbsp; &nbsp;0<br>35154 &nbsp; &nbsp;0<br>Name: label, Length: 35155, dtype: int64</span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras </span><span class="c3">import</span><span class="c4">&nbsp;metrics<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.callbacks </span><span class="c3">import</span><span class="c4">&nbsp;EarlyStopping<br>precision_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Precision()<br>recall_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Recall()<br>early_stopping </span><span class="c3">=</span><span class="c4">&nbsp;EarlyStopping(<br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;val_loss&#39;, &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Monitor validation loss</span><span class="c4"><br> &nbsp; &nbsp;patience</span><span class="c3">=</span><span class="c4">3, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Number of epochs with no improvement to wait</span><span class="c4"><br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c4 c13"># Print additional logs</span><span class="c4"><br> &nbsp; &nbsp;restore_best_weights</span><span class="c3">=True</span><span class="c4">&nbsp;</span><span class="c4 c13"># Restore the best weights when stopped</span><span class="c4"><br>)<br>model_checkpoint </span><span class="c3">=</span><span class="c4">&nbsp;ModelCheckpoint(<br> &nbsp; &nbsp;filepath</span><span class="c3">=</span><span class="c4">&#39;best_1_model.h5&#39;, &nbsp;</span><span class="c4 c13"># Replace with your desired file path and name</span><span class="c4"><br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;val_loss&#39;,<br> &nbsp; &nbsp;save_best_only</span><span class="c3">=True</span><span class="c4">,<br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1<br>)<br></span><span class="c4 c13"># Compile model</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">compile(<br> &nbsp; &nbsp;optimizer</span><span class="c3">=</span><span class="c4">&#39;adam&#39;,<br> &nbsp; &nbsp;loss</span><span class="c3">=</span><span class="c4">&#39;binary_crossentropy&#39;,<br> &nbsp; &nbsp;metrics</span><span class="c3">=</span><span class="c4">[<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">BinaryAccuracy(name</span><span class="c3">=</span><span class="c4">&#39;accuracy&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Precision(name</span><span class="c3">=</span><span class="c4">&#39;precision&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Recall(name</span><span class="c3">=</span><span class="c4">&#39;recall&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">AUC(name</span><span class="c3">=</span><span class="c4">&#39;auc&#39;)<br> &nbsp; &nbsp;]<br>)<br><br></span><span class="c4 c13"># Train the model</span><span class="c4"><br>history </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">fit(<br> &nbsp; &nbsp;X_train,<br> &nbsp; &nbsp;Y_label_train,<br> &nbsp; &nbsp;epochs</span><span class="c3">=</span><span class="c4">10,<br> &nbsp; &nbsp;batch_size</span><span class="c3">=</span><span class="c4">32,<br> &nbsp; &nbsp;validation_split</span><span class="c3">=</span><span class="c4">0.2,<br> &nbsp; &nbsp;callbacks</span><span class="c3">=</span><span class="c4">[tensorboard,early_stopping,model_checkpoint]<br>)<br><br></span><span class="c4 c13"># Evaluate the model</span><span class="c4"><br>loss, accuracy, precision, recall, auc </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">evaluate(X_test, Y_label_test)<br></span><span class="c3">if</span><span class="c4">&nbsp;precision </span><span class="c3">+</span><span class="c4">&nbsp;recall </span><span class="c3">==</span><span class="c4">&nbsp;0:<br> &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;0<br></span><span class="c3">else</span><span class="c4">:<br> &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;2 </span><span class="c3">*</span><span class="c4">&nbsp;(precision </span><span class="c3">*</span><span class="c4">&nbsp;recall) </span><span class="c3">/</span><span class="c4">&nbsp;(precision </span><span class="c3">+</span><span class="c2">&nbsp;recall)<br>print(f&quot;Accuracy: {accuracy}&quot;)<br>print(f&quot;Precision: {precision}&quot;)<br>print(f&quot;Recall: {recall}&quot;)<br>print(f&quot;F1 Score: {f1}&quot;)<br>print(f&quot;AUC: {auc}&quot;)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">Epoch 1/10<br>2015/2015 [==============================] - 21s 8ms/step - loss: 30.1738 - accuracy: 0.8352 - precision: 0.8719 - recall: 0.8955 - auc: 0.8303 - val_loss: 0.0331 - val_accuracy: 0.9880 - val_precision: 1.0000 - val_recall: 0.9880 - val_auc: 0.0000e+00<br>Epoch 2/10<br>2015/2015 [==============================] - 10s 5ms/step - loss: 1.0355 - accuracy: 0.8710 - precision: 0.8998 - recall: 0.9173 - auc: 0.9426 - val_loss: 0.0190 - val_accuracy: 0.9847 - val_precision: 1.0000 - val_recall: 0.9847 - val_auc: 0.0000e+00<br>Epoch 3/10<br>2015/2015 [==============================] - 11s 5ms/step - loss: 0.9998 - accuracy: 0.9004 - precision: 0.9717 - recall: 0.8830 - auc: 0.9585 - val_loss: 0.0330 - val_accuracy: 0.9656 - val_precision: 1.0000 - val_recall: 0.9656 - val_auc: 0.0000e+00<br>Epoch 4/10<br>2015/2015 [==============================] - 12s 6ms/step - loss: 0.3628 - accuracy: 0.8865 - precision: 0.9782 - recall: 0.8565 - auc: 0.9568 - val_loss: 0.0311 - val_accuracy: 0.9684 - val_precision: 1.0000 - val_recall: 0.9684 - val_auc: 0.0000e+00<br>Epoch 5/10<br>2009/2015 [============================&gt;.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8908 - precision: 0.9777 - recall: 0.8633 - auc: 0.9579Restoring model weights from the end of the best epoch: 2.<br></span></p><p class="c11 c30"><span class="c6"></span></p><p class="c11 c30"><span class="c6"></span></p><p class="c11"><span class="c6">Training</span></p><p class="c11"><span class="c6">2015/2015 [==============================] - 10s 5ms/step - loss: 0.2981 - accuracy: 0.8908 - precision: 0.9777 - recall: 0.8632 - auc: 0.9579 - val_loss: 0.0361 - val_accuracy: 0.9687 - val_precision: 1.0000 - val_recall: 0.9687 - val_auc: 0.0000e+00<br>Epoch 5: early stopping<br>1099/1099 [==============================] - 3s 3ms/step - loss: 0.5100 - </span></p><p class="c11"><span class="c6">Test</span></p><p class="c11"><span class="c6">accuracy: 0.9165 - precision: 0.9337 - recall: 0.9527 - auc: 0.9702<br>Accuracy: 0.9164556860923767<br>Precision: 0.9337194561958313<br>Recall: 0.9526847004890442<br>F1 Score: 0.9431067433926533<br>AUC: 0.9701996445655823<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4 c13"># Importing Libraries</span><span class="c4"><br></span><span class="c3">from</span><span class="c4">&nbsp;sklearn.metrics </span><span class="c3">import</span><span class="c4">&nbsp;recall_score, precision_score, f1_score, confusion_matrix<br><br></span><span class="c4 c13"># Prediction and Thresholding</span><span class="c4"><br>predictions </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">predict(X_test)<br>threshold </span><span class="c3">=</span><span class="c4">&nbsp;0.5<br>predicted_labels </span><span class="c3">=</span><span class="c4">&nbsp;(predictions </span><span class="c3">&gt;</span><span class="c4">&nbsp;threshold)</span><span class="c3">.</span><span class="c4">astype(int)<br><br></span><span class="c4 c13"># Ensure True Labels are in Correct Format</span><span class="c4"><br>true_labels </span><span class="c3">=</span><span class="c4">&nbsp;Y_label_test</span><span class="c3">.</span><span class="c4">to_numpy()<br><br></span><span class="c4 c13"># Compute Metrics</span><span class="c4"><br></span><span class="c4 c13"># Recall</span><span class="c4"><br>recall </span><span class="c3">=</span><span class="c4">&nbsp;recall_score(true_labels, predicted_labels)<br>print(f&quot;Recall: {recall}&quot;)<br><br></span><span class="c4 c13"># Precision</span><span class="c4"><br>precision </span><span class="c3">=</span><span class="c4">&nbsp;precision_score(true_labels, predicted_labels)<br>print(f&quot;Precision: {precision}&quot;)<br><br></span><span class="c4 c13"># F1 Score</span><span class="c4"><br>f1 </span><span class="c3">=</span><span class="c4">&nbsp;f1_score(true_labels, predicted_labels)<br>print(f&quot;F1 Score: {f1}&quot;)<br><br></span><span class="c4 c13"># Confusion Matrix</span><span class="c4"><br>conf_matrix </span><span class="c3">=</span><span class="c4">&nbsp;confusion_matrix(true_labels, predicted_labels)<br>print(f&quot;Confusion Matrix: \n{conf_matrix}&quot;)<br></span><span class="c3">from</span><span class="c4">&nbsp;sklearn.metrics </span><span class="c3">import</span><span class="c4">&nbsp;accuracy_score<br>accuracy </span><span class="c3">=</span><span class="c2">&nbsp;accuracy_score(true_labels, predicted_labels)<br>print(&quot;Test Accuracy:&quot;, accuracy)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">WARNING:tensorflow:6 out of the last 1104 calls to &lt;function Model.make_predict_function.&lt;locals&gt;.predict_function at 0x78e4f585eb90&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for &nbsp;more details.<br></span></p><p class="c9"><span class="c6"></span></p><p class="c14"><span class="c6"></span></p><p class="c11"><span class="c6">1099/1099 [==============================] - 3s 3ms/step<br>Recall: 0.952684721352536<br>Precision: 0.933719458402056<br>F1 Score: 0.9431067547411038<br>Confusion Matrix: <br>[[ 7875 &nbsp;1728]<br> [ 1209 24343]]<br>Test Accuracy: 0.9164556962025316<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">%</span><span class="c4">load_ext tensorboard<br></span><span class="c3">%</span><span class="c4">tensorboard </span><span class="c3">--</span><span class="c2">logdir logs<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">The tensorboard extension is already loaded. To reload it, use:<br> &nbsp;%reload_ext tensorboard<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c4">input_dim </span><span class="c3">=</span><span class="c4">&nbsp;X_train</span><span class="c3">.</span><span class="c4">shape[1]<br>output_dim_label </span><span class="c3">=</span><span class="c4">&nbsp;1 &nbsp;</span><span class="c4 c13"># Binary classification</span><span class="c4"><br>output_dim_attack_cat </span><span class="c3">=</span><span class="c4">&nbsp;len(attack_cat_cols) &nbsp;</span><span class="c4 c13"># Number of attack categories</span><span class="c4"><br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.callbacks </span><span class="c3">import</span><span class="c4">&nbsp;ModelCheckpoint<br><br></span><span class="c4 c13"># Define ModelCheckpoint</span><span class="c4"><br>model_checkpoint </span><span class="c3">=</span><span class="c4">&nbsp;ModelCheckpoint(<br> &nbsp; &nbsp;filepath</span><span class="c3">=</span><span class="c4">&#39;best_2_model.h5&#39;, &nbsp;</span><span class="c4 c13"># Replace with your desired file path and name</span><span class="c4"><br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;attack_cat_output_loss&#39;,<br> &nbsp; &nbsp;save_best_only</span><span class="c3">=True</span><span class="c4">,<br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1<br>)<br></span><span class="c4 c13"># Define the model</span><span class="c4"><br>model </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">models</span><span class="c3">.</span><span class="c4">Sequential([<br> &nbsp; &nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Dense(128, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;, input_shape</span><span class="c3">=</span><span class="c4">(input_dim,)),<br> &nbsp; &nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Dense(64, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;),<br> &nbsp; &nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Dense(32, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;),<br> &nbsp; &nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Dense(output_dim_label) &nbsp;</span><span class="c4 c13"># No activation here</span><span class="c4"><br>])<br><br></span><span class="c4 c13"># Custom activation split into two</span><span class="c4"><br></span><span class="c3">def</span><span class="c4">&nbsp;custom_activation(x):<br> &nbsp; &nbsp;sigmoid </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">activations</span><span class="c3">.</span><span class="c4">sigmoid(x[:, :output_dim_label])<br> &nbsp; &nbsp;softmax </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">activations</span><span class="c3">.</span><span class="c4">softmax(x[:, output_dim_label:])<br> &nbsp; &nbsp;</span><span class="c3">return</span><span class="c4">&nbsp;sigmoid </span><span class="c4 c13">## tf.concat([sigmoid, softmax], axis=1)</span><span class="c4"><br><br></span><span class="c4 c13"># Add custom activation layer</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">add(tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">layers</span><span class="c3">.</span><span class="c4">Lambda(custom_activation))<br><br><br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras </span><span class="c3">import</span><span class="c4">&nbsp;metrics<br><br>precision_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Precision()<br>recall_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Recall()<br><br></span><span class="c4 c13"># Compile model</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">compile(<br> &nbsp; &nbsp;optimizer</span><span class="c3">=</span><span class="c4">&#39;adam&#39;,<br> &nbsp; &nbsp;loss</span><span class="c3">=</span><span class="c4">&#39;binary_crossentropy&#39;,<br> &nbsp; &nbsp;metrics</span><span class="c3">=</span><span class="c4">[<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">BinaryAccuracy(name</span><span class="c3">=</span><span class="c4">&#39;accuracy&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Precision(name</span><span class="c3">=</span><span class="c4">&#39;precision&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">Recall(name</span><span class="c3">=</span><span class="c4">&#39;recall&#39;),<br> &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">.</span><span class="c4">AUC(name</span><span class="c3">=</span><span class="c4">&#39;auc&#39;)<br> &nbsp; &nbsp;]<br>)<br><br></span><span class="c4 c13"># Train the model</span><span class="c4"><br>history </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">fit(<br> &nbsp; &nbsp;X_train,<br> &nbsp; &nbsp;Y_label_train,<br> &nbsp; &nbsp;epochs</span><span class="c3">=</span><span class="c4">10,<br> &nbsp; &nbsp;batch_size</span><span class="c3">=</span><span class="c4">32,<br> &nbsp; &nbsp;validation_split</span><span class="c3">=</span><span class="c4">0.2,callbacks</span><span class="c3">=</span><span class="c4">[early_stopping,model_checkpoint]<br>)<br><br></span><span class="c4 c13"># Evaluate the model</span><span class="c4"><br>loss, accuracy, precision, recall, auc </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">evaluate(X_test, Y_label_test)<br></span><span class="c3">if</span><span class="c4">&nbsp;precision </span><span class="c3">+</span><span class="c4">&nbsp;recall </span><span class="c3">==</span><span class="c4">&nbsp;0:<br> &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;0<br></span><span class="c3">else</span><span class="c4">:<br> &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;2 </span><span class="c3">*</span><span class="c4">&nbsp;(precision </span><span class="c3">*</span><span class="c4">&nbsp;recall) </span><span class="c3">/</span><span class="c4">&nbsp;(precision </span><span class="c3">+</span><span class="c2">&nbsp;recall)<br>print(f&quot;Accuracy: {accuracy}&quot;)<br>print(f&quot;Precision: {precision}&quot;)<br>print(f&quot;Recall: {recall}&quot;)<br>print(f&quot;F1 Score: {f1}&quot;)<br>print(f&quot;AUC: {auc}&quot;)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">Epoch 1/10<br>2015/2015 [==============================] - 10s 4ms/step - loss: 39.0487 - accuracy: 0.8491 - precision: 0.8903 - recall: 0.8939 - auc: 0.8434 - val_loss: 1.5704 - val_accuracy: 0.9789 - val_precision: 1.0000 - val_recall: 0.9789 - val_auc: 0.0000e+00<br>Epoch 2/10<br>2015/2015 [==============================] - 11s 6ms/step - loss: 3.0833 - accuracy: 0.9168 - precision: 0.9328 - recall: 0.9492 - auc: 0.9492 - val_loss: 0.1512 - val_accuracy: 0.9800 - val_precision: 1.0000 - val_recall: 0.9800 - val_auc: 0.0000e+00<br>Epoch 3/10<br>2015/2015 [==============================] - 8s 4ms/step - loss: 1.4974 - accuracy: 0.9104 - precision: 0.9293 - recall: 0.9433 - auc: 0.9504 - val_loss: 0.0165 - val_accuracy: 0.9889 - val_precision: 1.0000 - val_recall: 0.9889 - val_auc: 0.0000e+00<br>Epoch 4/10<br>2015/2015 [==============================] - 10s 5ms/step - loss: 0.5514 - accuracy: 0.9286 - precision: 0.9729 - recall: 0.9235 - auc: 0.9741 - val_loss: 0.0812 - val_accuracy: 0.9806 - val_precision: 1.0000 - val_recall: 0.9806 - val_auc: 0.0000e+00<br>Epoch 5/10<br>2015/2015 [==============================] - 9s 5ms/step - loss: 0.2016 - accuracy: 0.9042 - precision: 0.9788 - recall: 0.8819 - auc: 0.9680 - val_loss: 0.0333 - val_accuracy: 0.9640 - val_precision: 1.0000 - val_recall: 0.9640 - val_auc: 0.0000e+00<br>Epoch 6/10<br>2015/2015 [==============================] - 8s 4ms/step - loss: 0.2124 - accuracy: 0.8797 - precision: 0.9677 - recall: 0.8562 - auc: 0.9575 - val_loss: 0.0241 - val_accuracy: 0.9716 - val_precision: 1.0000 - val_recall: 0.9716 - val_auc: 0.0000e+00<br>Epoch 7/10<br>2015/2015 [==============================] - 9s 4ms/step - loss: 0.2181 - accuracy: 0.8814 - precision: 0.9785 - recall: 0.8487 - auc: 0.9576 - val_loss: 0.0399 - val_accuracy: 0.9614 - val_precision: 1.0000 - val_recall: 0.9614 - val_auc: 0.0000e+00<br>Epoch 8/10<br>2015/2015 [==============================] - 9s 4ms/step - loss: 0.2010 - accuracy: 0.8918 - precision: 0.9798 - recall: 0.8627 - auc: 0.9598 - val_loss: 0.0336 - val_accuracy: 0.9678 - val_precision: 1.0000 - val_recall: 0.9678 - val_auc: 0.0000e+00<br>Epoch 9/10<br>2015/2015 [==============================] - 7s 4ms/step - loss: 0.5436 - accuracy: 0.8828 - precision: 0.9764 - recall: 0.8526 - auc: 0.9567 - val_loss: 0.0488 - val_accuracy: 0.9455 - val_precision: 1.0000 - val_recall: 0.9455 - val_auc: 0.0000e+00<br>Epoch 10/10<br>2015/2015 [==============================] - 9s 5ms/step - loss: 0.2151 - accuracy: 0.8859 - precision: 0.9794 - recall: 0.8545 - auc: 0.9580 - val_loss: 0.0298 - val_accuracy: 0.9710 - val_precision: 1.0000 - val_recall: 0.9710 - val_auc: 0.0000e+00<br>1099/1099 [==============================] - 3s 2ms/step - loss: 0.1832 - accuracy: 0.9039 - precision: 0.9513 - recall: 0.9147 - auc: 0.9694<br>Accuracy: 0.9039397239685059<br>Precision: 0.9512800574302673<br>Recall: 0.9146837592124939<br>F1 Score: 0.9326230350593969<br>AUC: 0.9694272875785828<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras </span><span class="c3">import</span><span class="c4">&nbsp;Model, Input<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.layers </span><span class="c3">import</span><span class="c4">&nbsp;Dense<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.optimizers </span><span class="c3">import</span><span class="c4">&nbsp;Adam<br><br></span><span class="c4 c13"># Define inputs</span><span class="c4"><br>input_layer </span><span class="c3">=</span><span class="c4">&nbsp;Input(shape</span><span class="c3">=</span><span class="c4">(X_train</span><span class="c3">.</span><span class="c4">shape[1],))<br><br></span><span class="c4 c13"># Define hidden layers</span><span class="c4"><br>hidden_layer </span><span class="c3">=</span><span class="c4">&nbsp;Dense(128, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(input_layer)<br>hidden_layer </span><span class="c3">=</span><span class="c4">&nbsp;Dense(64, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(hidden_layer)<br><br></span><span class="c4 c13"># Define outputs</span><span class="c4"><br>output_label </span><span class="c3">=</span><span class="c4">&nbsp;Dense(1, activation</span><span class="c3">=</span><span class="c4">&#39;sigmoid&#39;, name</span><span class="c3">=</span><span class="c4">&#39;label_output&#39;)(hidden_layer)<br>output_attack_cat </span><span class="c3">=</span><span class="c4">&nbsp;Dense(Y_attack_cat_train</span><span class="c3">.</span><span class="c4">shape[1], activation</span><span class="c3">=</span><span class="c4">&#39;softmax&#39;, name</span><span class="c3">=</span><span class="c4">&#39;attack_cat_output&#39;)(hidden_layer)<br><br></span><span class="c4 c13"># Compile model</span><span class="c4"><br>model </span><span class="c3">=</span><span class="c4">&nbsp;Model(inputs</span><span class="c3">=</span><span class="c4">input_layer, outputs</span><span class="c3">=</span><span class="c4">[output_label, output_attack_cat])<br>model</span><span class="c3">.</span><span class="c4">compile(optimizer</span><span class="c3">=</span><span class="c4">Adam(),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;loss</span><span class="c3">=</span><span class="c4">{&#39;label_output&#39;: &#39;binary_crossentropy&#39;, &#39;attack_cat_output&#39;: &#39;categorical_crossentropy&#39;},<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">=</span><span class="c4">[&#39;accuracy&#39;])<br><br></span><span class="c4 c13"># Train model</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">fit(X_train, {&#39;label_output&#39;: Y_label_train, &#39;attack_cat_output&#39;: Y_attack_cat_train}, epochs</span><span class="c3">=</span><span class="c4">10, batch_size</span><span class="c3">=</span><span class="c2">32)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">Epoch 1/10<br>2518/2518 [==============================] - 31s 9ms/step - loss: 266.8381 - label_output_loss: 31.9629 - attack_cat_output_loss: 234.8747 - label_output_accuracy: 0.8678 - attack_cat_output_accuracy: 0.7695<br>Epoch 2/10<br>2518/2518 [==============================] - 26s 10ms/step - loss: 7.3855 - label_output_loss: 0.7574 - attack_cat_output_loss: 6.6280 - label_output_accuracy: 0.9021 - attack_cat_output_accuracy: 0.8210<br>Epoch 3/10<br>2518/2518 [==============================] - 17s 7ms/step - loss: 1.5420 - label_output_loss: 0.1560 - attack_cat_output_loss: 1.3860 - label_output_accuracy: 0.9424 - attack_cat_output_accuracy: 0.8802<br>Epoch 4/10<br>2518/2518 [==============================] - 10s 4ms/step - loss: 0.6071 - label_output_loss: 0.1959 - attack_cat_output_loss: 0.4112 - label_output_accuracy: 0.8786 - attack_cat_output_accuracy: 0.8319<br>Epoch 5/10<br>2518/2518 [==============================] - 9s 3ms/step - loss: 0.5759 - label_output_loss: 0.1804 - attack_cat_output_loss: 0.3955 - label_output_accuracy: 0.8966 - attack_cat_output_accuracy: 0.8523<br>Epoch 6/10<br>2518/2518 [==============================] - 10s 4ms/step - loss: 0.5565 - label_output_loss: 0.1766 - attack_cat_output_loss: 0.3798 - label_output_accuracy: 0.8999 - attack_cat_output_accuracy: 0.8569<br>Epoch 7/10<br>2518/2518 [==============================] - 8s 3ms/step - loss: 0.5501 - label_output_loss: 0.1789 - attack_cat_output_loss: 0.3713 - label_output_accuracy: 0.8931 - attack_cat_output_accuracy: 0.8513<br>Epoch 8/10<br>2518/2518 [==============================] - 10s 4ms/step - loss: 0.6755 - label_output_loss: 0.2069 - attack_cat_output_loss: 0.4686 - label_output_accuracy: 0.8805 - attack_cat_output_accuracy: 0.8313<br>Epoch 9/10<br>2518/2518 [==============================] - 10s 4ms/step - loss: 0.5616 - label_output_loss: 0.1845 - attack_cat_output_loss: 0.3771 - label_output_accuracy: 0.8821 - attack_cat_output_accuracy: 0.8447<br>Epoch 10/10<br>2518/2518 [==============================] - 9s 4ms/step - loss: 0.5844 - label_output_loss: 0.1854 - attack_cat_output_loss: 0.3990 - label_output_accuracy: 0.8975 - attack_cat_output_accuracy: 0.8564<br></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14"><span class="c12"></span></p><p class="c11"><span class="c6">&lt;keras.src.callbacks.History at 0x78e5167ea7a0&gt;</span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;sklearn.metrics </span><span class="c3">import</span><span class="c4">&nbsp;confusion_matrix, classification_report<br></span><span class="c3">import</span><span class="c4">&nbsp;numpy </span><span class="c3">as</span><span class="c4">&nbsp;np<br><br></span><span class="c4 c13"># Predict</span><span class="c4"><br>Y_label_pred, Y_attack_cat_pred </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">predict(X_test)<br><br></span><span class="c4 c13"># Convert softmax probabilities to labels for attack categories</span><span class="c4"><br>Y_attack_cat_pred_labels </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">argmax(Y_attack_cat_pred, axis</span><span class="c3">=</span><span class="c4">1)<br>Y_attack_cat_true_labels </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">argmax(np</span><span class="c3">.</span><span class="c4">array(Y_attack_cat_test), axis</span><span class="c3">=</span><span class="c4">1)<br><br></span><span class="c4 c13"># Convert sigmoid output to labels for binary classification</span><span class="c4"><br>Y_label_pred_labels </span><span class="c3">=</span><span class="c4">&nbsp;(Y_label_pred </span><span class="c3">&gt;</span><span class="c4">&nbsp;0.5)</span><span class="c3">.</span><span class="c4">astype(int)<br><br></span><span class="c4 c13"># Confusion Matrix</span><span class="c4"><br>conf_matrix_label </span><span class="c3">=</span><span class="c4">&nbsp;confusion_matrix(Y_label_test, Y_label_pred_labels)<br>conf_matrix_attack_cat </span><span class="c3">=</span><span class="c4">&nbsp;confusion_matrix(Y_attack_cat_true_labels, Y_attack_cat_pred_labels)<br><br>print(&quot;Confusion Matrix for Label:&quot;)<br>print(conf_matrix_label)<br><br>print(&quot;Confusion Matrix for Attack Categories:&quot;)<br>print(conf_matrix_attack_cat)<br><br></span><span class="c4 c13"># Classification Report (precision, recall, f1-score)</span><span class="c4"><br>report_label </span><span class="c3">=</span><span class="c4">&nbsp;classification_report(Y_label_test, Y_label_pred_labels, target_names</span><span class="c3">=</span><span class="c4">[&#39;Class 0&#39;, &#39;Class 1&#39;])<br>report_attack_cat </span><span class="c3">=</span><span class="c2">&nbsp;classification_report(Y_attack_cat_true_labels, Y_attack_cat_pred_labels)<br><br>print(&quot;Classification Report for Label:&quot;)<br>print(report_label)<br><br>print(&quot;Classification Report for Attack Categories:&quot;)<br>print(report_attack_cat)<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">1099/1099 [==============================] - 4s 3ms/step<br>Confusion Matrix for Label:<br>[[ 7433 &nbsp;2170]<br> [ 4372 21180]]<br>Confusion Matrix for Attack Categories:<br>[[ &nbsp; &nbsp;0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 9 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 2 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0]<br> [ &nbsp; &nbsp;0 &nbsp; &nbsp; 6 &nbsp; 281 &nbsp; &nbsp; 3 &nbsp; &nbsp; 7 &nbsp; 384 &nbsp; &nbsp;36 &nbsp; &nbsp; 0]<br> [ &nbsp; &nbsp;0 &nbsp; &nbsp; 0 &nbsp;1607 &nbsp; &nbsp; 5 &nbsp; &nbsp;26 &nbsp;3534 &nbsp; 121 &nbsp; &nbsp; 0]<br> [ &nbsp; &nbsp;0 &nbsp; &nbsp;17 &nbsp; &nbsp;92 &nbsp; 179 &nbsp; &nbsp; 0 &nbsp; 227 &nbsp; &nbsp;18 &nbsp; &nbsp; 0]<br> [ &nbsp; &nbsp;0 &nbsp; &nbsp; 0 &nbsp; &nbsp;35 &nbsp; &nbsp; 2 18149 &nbsp; 272 &nbsp; &nbsp; 2 &nbsp; &nbsp; 0]<br> [ &nbsp; &nbsp;0 &nbsp; &nbsp; 2 &nbsp;1714 &nbsp; 218 &nbsp; &nbsp; 3 &nbsp;7654 &nbsp; &nbsp;12 &nbsp; &nbsp; 0]<br> [ &nbsp; &nbsp;0 &nbsp; &nbsp; 0 &nbsp; 243 &nbsp; &nbsp; 0 &nbsp; &nbsp; 5 &nbsp; &nbsp; 7 &nbsp; 249 &nbsp; &nbsp; 0]<br> [ &nbsp; &nbsp;0 &nbsp; &nbsp; 0 &nbsp; &nbsp;25 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0 &nbsp; &nbsp; 9 &nbsp; &nbsp; 0 &nbsp; &nbsp; 0]]<br>Classification Report for Label:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;precision &nbsp; &nbsp;recall &nbsp;f1-score &nbsp; support<br><br> &nbsp; &nbsp; Class 0 &nbsp; &nbsp; &nbsp; 0.63 &nbsp; &nbsp; &nbsp;0.77 &nbsp; &nbsp; &nbsp;0.69 &nbsp; &nbsp; &nbsp;9603<br> &nbsp; &nbsp; Class 1 &nbsp; &nbsp; &nbsp; 0.91 &nbsp; &nbsp; &nbsp;0.83 &nbsp; &nbsp; &nbsp;0.87 &nbsp; &nbsp; 25552<br><br> &nbsp; &nbsp;accuracy &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.81 &nbsp; &nbsp; 35155<br> &nbsp; macro avg &nbsp; &nbsp; &nbsp; 0.77 &nbsp; &nbsp; &nbsp;0.80 &nbsp; &nbsp; &nbsp;0.78 &nbsp; &nbsp; 35155<br>weighted avg &nbsp; &nbsp; &nbsp; 0.83 &nbsp; &nbsp; &nbsp;0.81 &nbsp; &nbsp; &nbsp;0.82 &nbsp; &nbsp; 35155<br><br>Classification Report for Attack Categories:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;precision &nbsp; &nbsp;recall &nbsp;f1-score &nbsp; support<br><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0 &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; &nbsp;0.00 &nbsp; &nbsp; &nbsp;0.00 &nbsp; &nbsp; &nbsp; &nbsp;11<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1 &nbsp; &nbsp; &nbsp; 0.24 &nbsp; &nbsp; &nbsp;0.01 &nbsp; &nbsp; &nbsp;0.02 &nbsp; &nbsp; &nbsp; 717<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 2 &nbsp; &nbsp; &nbsp; 0.40 &nbsp; &nbsp; &nbsp;0.30 &nbsp; &nbsp; &nbsp;0.35 &nbsp; &nbsp; &nbsp;5293<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 3 &nbsp; &nbsp; &nbsp; 0.44 &nbsp; &nbsp; &nbsp;0.34 &nbsp; &nbsp; &nbsp;0.38 &nbsp; &nbsp; &nbsp; 533<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 4 &nbsp; &nbsp; &nbsp; 1.00 &nbsp; &nbsp; &nbsp;0.98 &nbsp; &nbsp; &nbsp;0.99 &nbsp; &nbsp; 18460<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 5 &nbsp; &nbsp; &nbsp; 0.63 &nbsp; &nbsp; &nbsp;0.80 &nbsp; &nbsp; &nbsp;0.71 &nbsp; &nbsp; &nbsp;9603<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 6 &nbsp; &nbsp; &nbsp; 0.57 &nbsp; &nbsp; &nbsp;0.49 &nbsp; &nbsp; &nbsp;0.53 &nbsp; &nbsp; &nbsp; 504<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 7 &nbsp; &nbsp; &nbsp; 0.00 &nbsp; &nbsp; &nbsp;0.00 &nbsp; &nbsp; &nbsp;0.00 &nbsp; &nbsp; &nbsp; &nbsp;34<br><br> &nbsp; &nbsp;accuracy &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.79 &nbsp; &nbsp; 35155<br> &nbsp; macro avg &nbsp; &nbsp; &nbsp; 0.41 &nbsp; &nbsp; &nbsp;0.37 &nbsp; &nbsp; &nbsp;0.37 &nbsp; &nbsp; 35155<br>weighted avg &nbsp; &nbsp; &nbsp; 0.78 &nbsp; &nbsp; &nbsp;0.79 &nbsp; &nbsp; &nbsp;0.78 &nbsp; &nbsp; 35155<br><br></span></p><p class="c9"><span class="c6"></span></p><p class="c14"><span class="c6"></span></p><p class="c11"><span class="c6">/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.<br> &nbsp;_warn_prf(average, modifier, msg_start, len(result))<br>/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.<br> &nbsp;_warn_prf(average, modifier, msg_start, len(result))<br>/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.<br> &nbsp;_warn_prf(average, modifier, msg_start, len(result))<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9 c19"><span class="c6"></span></p><h2 class="c21 c19"><span class="c27 c28"></span></h2><p class="c23 c19"><span>What is real anymore ?, GANS to the rescue.....maybe</span><span class="c29"><a class="c26" href="#id.1t3h5sf">&para;</a></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c2">df<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14 c19"><span class="c12"></span></p><p class="c14 c19"><span class="c12"></span></p><a id="t.c8e2d79c65f27fea83fa53135fc03fb03221e679"></a><a id="t.6"></a><table class="c7"><thead><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c25"><span class="c12"></span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">ackdat</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dbytes</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dinpkt</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">djit</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dloss</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dmean</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dpkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dtcpb</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">dur</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sinpkt</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sjit</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sload</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">sloss</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">smean</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">spkts</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">stcpb</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">swin</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">synack</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c15"><span class="c10">tcprtt</span></p><p class="c22 c19"><span class="c1">&nbsp;</span></p></td><tbody></tbody></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">770</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.076256</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3.232083</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6.951521</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">64</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.724011</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.387386</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.187251</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">3.686138</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10.554484</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">52</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.739236</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.021395</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2212</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.045820</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.600905</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">7.584815</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">79</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">28</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.656690</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.399717</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.913582</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.542055</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">19.352890</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">28</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">909</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">62</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.906903</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.018450</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.039412</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.020613</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1096</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.960645</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.491322</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8.186304</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">137</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">8</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.082292</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.231460</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.968896</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.420881</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.775308</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">86</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.152212</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.018843</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.038855</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">3</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.021823</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">268</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.994504</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.529934</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">7.280236</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">45</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.124919</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.211707</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">1.919755</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.392143</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14.372310</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">100</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">10</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">13.031262</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.018604</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.040243</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">4</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.020529</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">950</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.004668</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.465016</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">7.499679</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">6</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">79</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">11.667646</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.341113</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2.073796</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">4.589100</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">11.729767</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">5</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">49</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">14</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">12.703499</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">255</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.018431</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.038355</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80565</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000006</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.005975</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">36.394263</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">57</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80566</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.008944</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">34.983254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">57</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80567</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.008944</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">34.983254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">57</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80568</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.008944</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">34.983254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">57</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td></tr><tr class="c20"><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c10">80569</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000009</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">...</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.008944</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">34.983254</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">57</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">2</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td><td class="c0" colspan="1" rowspan="1"><p class="c5"><span class="c1">0.000000</span></p></td></tr></thead></table><p class="c24 c19"><span class="c12">80570 rows &times; 420 columns</span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c16"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c9"><span class="c12"></span></p><p class="c18"><span class="c12">Out[ ]:</span></p><p class="c14"><span class="c12"></span></p><p class="c11"><span class="c6">(80570, 420)</span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;keras.models </span><span class="c3">import</span><span class="c4">&nbsp;Sequential, Model<br></span><span class="c3">from</span><span class="c4">&nbsp;keras.layers </span><span class="c3">import</span><span class="c4">&nbsp;Dense, LeakyReLU, BatchNormalization, Input, Concatenate<br></span><span class="c3">from</span><span class="c4">&nbsp;keras.optimizers </span><span class="c3">import</span><span class="c4">&nbsp;Adam<br></span><span class="c3">import</span><span class="c4">&nbsp;numpy </span><span class="c3">as</span><span class="c4">&nbsp;np<br><br></span><span class="c4 c13"># Prepare your data: df for features, labels for attack categories</span><span class="c4"><br></span><span class="c4 c13"># df = ...</span><span class="c4"><br></span><span class="c4 c13"># labels = ...</span><span class="c4"><br>best_g_loss </span><span class="c3">=</span><span class="c4">&nbsp;float(&#39;inf&#39;) &nbsp;</span><span class="c4 c13"># Initialize best loss to infinity</span><span class="c4"><br>target_g_loss </span><span class="c3">=</span><span class="c4">&nbsp;3.0 &nbsp;</span><span class="c4 c13"># The target G loss value for early stopping</span><span class="c4"><br>patience </span><span class="c3">=</span><span class="c4">&nbsp;330 &nbsp;</span><span class="c4 c13"># Number of epochs to wait for improvement</span><span class="c4"><br>wait </span><span class="c3">=</span><span class="c4">&nbsp;0 &nbsp;</span><span class="c4 c13"># Initialize wait counter to 0</span><span class="c4"><br><br></span><span class="c4 c13"># Number of attack categories</span><span class="c4"><br>num_attack_categories </span><span class="c3">=</span><span class="c4">&nbsp;8<br><br></span><span class="c4 c13"># Define condition shape</span><span class="c4"><br>condition_shape </span><span class="c3">=</span><span class="c4">&nbsp;(num_attack_categories,)<br><br></span><span class="c4 c13"># Build Generator</span><span class="c4"><br>generator </span><span class="c3">=</span><span class="c4">&nbsp;Sequential()<br>generator</span><span class="c3">.</span><span class="c4">add(Dense(128, input_shape</span><span class="c3">=</span><span class="c4">(100 </span><span class="c3">+</span><span class="c4">&nbsp;num_attack_categories,)))<br>generator</span><span class="c3">.</span><span class="c4">add(LeakyReLU(0.01))<br>generator</span><span class="c3">.</span><span class="c4">add(BatchNormalization())<br>generator</span><span class="c3">.</span><span class="c4">add(Dense(X_train</span><span class="c3">.</span><span class="c4">shape[1]))<br>generator</span><span class="c3">.</span><span class="c4">add(LeakyReLU(0.01))<br><br></span><span class="c4 c13"># Build Discriminator</span><span class="c4"><br>discriminator </span><span class="c3">=</span><span class="c4">&nbsp;Sequential()<br>discriminator</span><span class="c3">.</span><span class="c4">add(Dense(128, input_shape</span><span class="c3">=</span><span class="c4">(X_train</span><span class="c3">.</span><span class="c4">shape[1]</span><span class="c3">+</span><span class="c4">8,)))<br>discriminator</span><span class="c3">.</span><span class="c4">add(LeakyReLU(0.01))<br>discriminator</span><span class="c3">.</span><span class="c4">add(Dense(1, activation</span><span class="c3">=</span><span class="c4">&#39;sigmoid&#39;))<br><br></span><span class="c4 c13"># Compile Discriminator</span><span class="c4"><br>discriminator</span><span class="c3">.</span><span class="c4">compile(optimizer</span><span class="c3">=</span><span class="c4">Adam(), loss</span><span class="c3">=</span><span class="c4">&#39;binary_crossentropy&#39;, metrics</span><span class="c3">=</span><span class="c4">[&#39;accuracy&#39;])<br><br></span><span class="c4 c13"># Combined Network</span><span class="c4"><br>discriminator</span><span class="c3">.</span><span class="c4">trainable </span><span class="c3">=</span><span class="c4">&nbsp;</span><span class="c3">False</span><span class="c4"><br><br>gan_input </span><span class="c3">=</span><span class="c4">&nbsp;Input(shape</span><span class="c3">=</span><span class="c4">(100,))<br>condition_input </span><span class="c3">=</span><span class="c4">&nbsp;Input(shape</span><span class="c3">=</span><span class="c4">condition_shape)<br><br>x_gen </span><span class="c3">=</span><span class="c4">&nbsp;Concatenate(axis</span><span class="c3">=-</span><span class="c4">1)([gan_input, condition_input])<br>x_gen </span><span class="c3">=</span><span class="c4">&nbsp;generator(x_gen)<br><br>conditioned_x_gen </span><span class="c3">=</span><span class="c4">&nbsp;Concatenate(axis</span><span class="c3">=-</span><span class="c4">1)([x_gen, condition_input])<br>gan_output </span><span class="c3">=</span><span class="c4">&nbsp;discriminator(conditioned_x_gen)<br><br>gan </span><span class="c3">=</span><span class="c4">&nbsp;Model([gan_input, condition_input], gan_output)<br>gan</span><span class="c3">.</span><span class="c4">compile(optimizer</span><span class="c3">=</span><span class="c4">Adam(), loss</span><span class="c3">=</span><span class="c4">&#39;binary_crossentropy&#39;, metrics</span><span class="c3">=</span><span class="c4">[&#39;accuracy&#39;])<br><br></span><span class="c4 c13"># Example of how to train</span><span class="c4"><br>batch_size </span><span class="c3">=</span><span class="c4">&nbsp;32<br>epochs </span><span class="c3">=</span><span class="c4">&nbsp;1000<br>half_batch </span><span class="c3">=</span><span class="c4">&nbsp;batch_size </span><span class="c3">//</span><span class="c4">&nbsp;2<br><br></span><span class="c3">for</span><span class="c4">&nbsp;epoch </span><span class="c3">in</span><span class="c4">&nbsp;range(epochs):<br> &nbsp; &nbsp;</span><span class="c4 c13"># Train Discriminator</span><span class="c4"><br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Randomly select a half-batch of real data points and their labels</span><span class="c4"><br> &nbsp; &nbsp;idx </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">random</span><span class="c3">.</span><span class="c4">randint(0, X_train</span><span class="c3">.</span><span class="c4">shape[0], half_batch)<br> &nbsp; &nbsp;real_data </span><span class="c3">=</span><span class="c4">&nbsp;X_train</span><span class="c3">.</span><span class="c4">iloc[idx]<br> &nbsp; &nbsp;real_attack_categories </span><span class="c3">=</span><span class="c4">&nbsp;Y_attack_cat_train[idx]<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Generate a half-batch of fake data points</span><span class="c4"><br> &nbsp; &nbsp;noise </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">random</span><span class="c3">.</span><span class="c4">normal(0, 1, (half_batch, 100))<br> &nbsp; &nbsp;interested_category </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">array([[0, 1, 0, 0, 0, 0, 0, 0]] </span><span class="c3">*</span><span class="c4">&nbsp;half_batch) &nbsp;</span><span class="c4 c13"># One-hot encoding for category 1</span><span class="c4"><br><br> &nbsp; &nbsp;combined_input </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">concatenate([noise, interested_category], axis</span><span class="c3">=</span><span class="c4">1)<br> &nbsp; &nbsp;fake_data </span><span class="c3">=</span><span class="c4">&nbsp;generator</span><span class="c3">.</span><span class="c4">predict(combined_input)<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Train the discriminator</span><span class="c4"><br> &nbsp; &nbsp;real_labels </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">ones((half_batch, 1))<br> &nbsp; &nbsp;fake_labels </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">zeros((half_batch, 1))<br> &nbsp; &nbsp;</span><span class="c4 c13"># Convert DataFrame to numpy array if needed</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;isinstance(real_data, pd</span><span class="c3">.</span><span class="c4">DataFrame):<br> &nbsp; &nbsp; &nbsp; &nbsp;real_data </span><span class="c3">=</span><span class="c4">&nbsp;real_data</span><span class="c3">.</span><span class="c4">to_numpy()<br> &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;isinstance(Y_attack_cat_train, pd</span><span class="c3">.</span><span class="c4">DataFrame):<br> &nbsp; &nbsp; &nbsp; &nbsp;Y_attack_cat_train </span><span class="c3">=</span><span class="c4">&nbsp;Y_attack_cat_train</span><span class="c3">.</span><span class="c4">to_numpy()<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Concatenate along columns (axis=1)</span><span class="c4"><br><br> &nbsp; &nbsp;concatenated_data </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">concatenate([real_data, real_attack_categories], axis</span><span class="c3">=</span><span class="c4">1)<br><br> &nbsp; &nbsp;d_loss_real </span><span class="c3">=</span><span class="c4">&nbsp;discriminator</span><span class="c3">.</span><span class="c4">train_on_batch(concatenated_data, real_labels)<br><br><br><br> &nbsp; &nbsp;concatenated_data </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">concatenate([fake_data, interested_category], axis</span><span class="c3">=</span><span class="c4">1)<br><br> &nbsp; &nbsp;d_loss_fake </span><span class="c3">=</span><span class="c4">&nbsp;discriminator</span><span class="c3">.</span><span class="c4">train_on_batch(concatenated_data, fake_labels)<br><br> &nbsp; &nbsp;d_loss </span><span class="c3">=</span><span class="c4">&nbsp;0.5 </span><span class="c3">*</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">add(d_loss_real, d_loss_fake)<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Train Generator</span><span class="c4"><br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Generate noise for a full batch</span><span class="c4"><br> &nbsp; &nbsp;noise </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">random</span><span class="c3">.</span><span class="c4">normal(0, 1, (batch_size, 100))<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Condition on the interested category for a full batch</span><span class="c4"><br> &nbsp; &nbsp;interested_category </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">array([[0, 1, 0, 0, 0, 0, 0, 0]] </span><span class="c3">*</span><span class="c4">&nbsp;batch_size)<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Train the generator</span><span class="c4"><br> &nbsp; &nbsp;g_loss </span><span class="c3">=</span><span class="c4">&nbsp;gan</span><span class="c3">.</span><span class="c4">train_on_batch([noise, interested_category], np</span><span class="c3">.</span><span class="c4">ones((batch_size, 1)))<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Print the losses</span><span class="c4"><br> &nbsp; &nbsp;print(f&quot;{epoch} [D loss: {d_loss[0]} | D Accuracy: {100</span><span class="c3">*</span><span class="c4">d_loss[1]}] [G loss: {g_loss}]&quot;)<br> &nbsp; &nbsp;val_g_loss </span><span class="c3">=</span><span class="c4">&nbsp;g_loss[0]<br><br> &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;val_g_loss </span><span class="c3">&lt;</span><span class="c4">&nbsp;best_g_loss:<br> &nbsp; &nbsp; &nbsp; &nbsp;best_g_loss </span><span class="c3">=</span><span class="c4">&nbsp;val_g_loss &nbsp;</span><span class="c4 c13"># Update best loss</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;wait </span><span class="c3">=</span><span class="c4">&nbsp;0 &nbsp;</span><span class="c4 c13"># Reset wait counter</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Save model parameters</span><span class="c4"><br><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Check if G loss has reached the target</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;best_g_loss </span><span class="c3">&lt;</span><span class="c4">&nbsp;target_g_loss:<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(f&quot;Target G loss reached on epoch {epoch}&quot;)<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Load best model parameters</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c3">break</span><span class="c4"><br> &nbsp; &nbsp;</span><span class="c3">else</span><span class="c4">:<br> &nbsp; &nbsp; &nbsp; &nbsp;wait </span><span class="c3">+=</span><span class="c4">&nbsp;1<br><br> &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;wait </span><span class="c3">&gt;=</span><span class="c4">&nbsp;patience:<br> &nbsp; &nbsp; &nbsp; &nbsp;print(f&quot;Early stopping on epoch {epoch}&quot;)<br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Load best model parameters</span><span class="c4"><br> &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c3">break</span><span class="c2"><br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">1/1 [==============================] - 0s 74ms/step<br>0 [D loss: 18.07179617881775 | D Accuracy: 34.375] [G loss: [0.7821332216262817, 0.40625]]<br>Target G loss reached on epoch 0<br></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c9"><span class="c6"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">import</span><span class="c4">&nbsp;numpy </span><span class="c3">as</span><span class="c4">&nbsp;np<br><br></span><span class="c3">def</span><span class="c4">&nbsp;generate_data(generator, num_samples):<br> &nbsp; &nbsp;&quot;&quot;&quot;<br> &nbsp; &nbsp;Generate `num_samples` data samples using the trained generator model.<br> &nbsp; &nbsp;&quot;&quot;&quot;<br> &nbsp; &nbsp;</span><span class="c4 c13"># Generate random noise as input for the generator</span><span class="c4"><br> &nbsp; &nbsp;noise </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">random</span><span class="c3">.</span><span class="c4">normal(0, 1, (num_samples, generator</span><span class="c3">.</span><span class="c4">input_shape[1]))<br><br> &nbsp; &nbsp;</span><span class="c4 c13"># Generate synthetic data samples</span><span class="c4"><br> &nbsp; &nbsp;generated_data </span><span class="c3">=</span><span class="c4">&nbsp;generator</span><span class="c3">.</span><span class="c4">predict(noise)<br><br> &nbsp; &nbsp;</span><span class="c3">return</span><span class="c2">&nbsp;generated_data<br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c18"><span class="c12">In [&nbsp;]:</span></p><p class="c9"><span class="c12"></span></p><p class="c14 c17"><span class="c12"></span></p><p class="c8"><span class="c12"></span></p><p class="c11 c17"><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.callbacks </span><span class="c3">import</span><span class="c4">&nbsp;ModelCheckpoint<br><br></span><span class="c4 c13"># Define ModelCheckpoint</span><span class="c4"><br>model_checkpoint </span><span class="c3">=</span><span class="c4">&nbsp;ModelCheckpoint(<br> &nbsp; &nbsp;filepath</span><span class="c3">=</span><span class="c4">&#39;best_gen_model.h5&#39;, &nbsp;</span><span class="c4 c13"># Replace with your desired file path and name</span><span class="c4"><br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;attack_cat_output_loss&#39;,<br> &nbsp; &nbsp;save_best_only</span><span class="c3">=True</span><span class="c4">,<br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1<br>)<br><br></span><span class="c4 c13"># Assuming `generator` is your trained generator model</span><span class="c4"><br>num_samples </span><span class="c3">=</span><span class="c4">&nbsp;2000 &nbsp;</span><span class="c4 c13"># The number of artificial data samples you want to generate</span><span class="c4"><br>artificial_data </span><span class="c3">=</span><span class="c4">&nbsp;generate_data(generator, num_samples)<br><br>artificial_data</span><span class="c3">.</span><span class="c4">shape<br>interested_category </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">array([[0, 1, 0, 0, 0, 0, 0, 0]] </span><span class="c3">*</span><span class="c4">&nbsp;num_samples)<br>y_label_gen</span><span class="c3">=</span><span class="c4">np</span><span class="c3">.</span><span class="c4">ones((num_samples, 1))<br><br>X_train_2</span><span class="c3">=</span><span class="c4">np</span><span class="c3">.</span><span class="c4">concatenate([X_train, artificial_data], axis</span><span class="c3">=</span><span class="c4">0)<br><br>y_label_gen_reshaped </span><span class="c3">=</span><span class="c4">&nbsp;y_label_gen</span><span class="c3">.</span><span class="c4">ravel() &nbsp;</span><span class="c4 c13"># Making it 1D</span><span class="c4"><br>Y_label_train_2 </span><span class="c3">=</span><span class="c4">&nbsp;np</span><span class="c3">.</span><span class="c4">concatenate([Y_label_train, y_label_gen_reshaped], axis</span><span class="c3">=</span><span class="c4">0)<br><br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras </span><span class="c3">import</span><span class="c4">&nbsp;metrics<br></span><span class="c3">from</span><span class="c4">&nbsp;tensorflow.keras.callbacks </span><span class="c3">import</span><span class="c4">&nbsp;EarlyStopping<br>precision_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Precision()<br>recall_metric </span><span class="c3">=</span><span class="c4">&nbsp;tf</span><span class="c3">.</span><span class="c4">keras</span><span class="c3">.</span><span class="c4">metrics</span><span class="c3">.</span><span class="c4">Recall()<br>early_stopping </span><span class="c3">=</span><span class="c4">&nbsp;EarlyStopping(<br> &nbsp; &nbsp;monitor</span><span class="c3">=</span><span class="c4">&#39;attack_cat_output_loss&#39;, &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Monitor validation loss</span><span class="c4"><br> &nbsp; &nbsp;patience</span><span class="c3">=</span><span class="c4">4, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c4 c13"># Number of epochs with no improvement to wait</span><span class="c4"><br> &nbsp; &nbsp;verbose</span><span class="c3">=</span><span class="c4">1, &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c4 c13"># Print additional logs</span><span class="c4"><br> &nbsp; &nbsp;restore_best_weights</span><span class="c3">=True</span><span class="c4">&nbsp;</span><span class="c4 c13"># Restore the best weights when stopped</span><span class="c4"><br>)<br><br><br><br><br></span><span class="c4 c13"># Define inputs</span><span class="c4"><br>input_layer </span><span class="c3">=</span><span class="c4">&nbsp;Input(shape</span><span class="c3">=</span><span class="c4">(X_train</span><span class="c3">.</span><span class="c4">shape[1],))<br><br></span><span class="c4 c13"># Define hidden layers</span><span class="c4"><br>hidden_layer </span><span class="c3">=</span><span class="c4">&nbsp;Dense(128, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(input_layer)<br>hidden_layer </span><span class="c3">=</span><span class="c4">&nbsp;Dense(64, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(hidden_layer)<br>hidden_layer </span><span class="c3">=</span><span class="c4">&nbsp;Dense(32, activation</span><span class="c3">=</span><span class="c4">&#39;relu&#39;)(hidden_layer)<br></span><span class="c4 c13"># Define outputs</span><span class="c4"><br>output_label </span><span class="c3">=</span><span class="c4">&nbsp;Dense(1, activation</span><span class="c3">=</span><span class="c4">&#39;sigmoid&#39;, name</span><span class="c3">=</span><span class="c4">&#39;label_output&#39;)(hidden_layer)<br>output_attack_cat </span><span class="c3">=</span><span class="c4">&nbsp;Dense(Y_attack_cat_train</span><span class="c3">.</span><span class="c4">shape[1], activation</span><span class="c3">=</span><span class="c4">&#39;softmax&#39;, name</span><span class="c3">=</span><span class="c4">&#39;attack_cat_output&#39;)(hidden_layer)<br><br></span><span class="c4 c13"># Compile model</span><span class="c4"><br>model </span><span class="c3">=</span><span class="c4">&nbsp;Model(inputs</span><span class="c3">=</span><span class="c4">input_layer, outputs</span><span class="c3">=</span><span class="c4">[output_label, output_attack_cat])<br>model</span><span class="c3">.</span><span class="c4">compile(optimizer</span><span class="c3">=</span><span class="c4">Adam(),<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;loss</span><span class="c3">=</span><span class="c4">{&#39;label_output&#39;: &#39;binary_crossentropy&#39;, &#39;attack_cat_output&#39;: &#39;categorical_crossentropy&#39;},<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;metrics</span><span class="c3">=</span><span class="c4">[&#39;accuracy&#39;])<br><br></span><span class="c4 c13"># Train model</span><span class="c4"><br>model</span><span class="c3">.</span><span class="c4">fit(X_train, {&#39;label_output&#39;: Y_label_train, &#39;attack_cat_output&#39;: Y_attack_cat_train}, epochs</span><span class="c3">=</span><span class="c4">10, batch_size</span><span class="c3">=</span><span class="c4">32,callbacks</span><span class="c3">=</span><span class="c4">[early_stopping,model_checkpoint])<br><br><br><br></span><span class="c4 c13"># Evaluate the model</span><span class="c4"><br></span><span class="c3">try</span><span class="c4">:<br> &nbsp; &nbsp;loss, accuracy, precision, recall, auc </span><span class="c3">=</span><span class="c4">&nbsp;model</span><span class="c3">.</span><span class="c4">evaluate(X_test, Y_label_test)<br> &nbsp; &nbsp;</span><span class="c3">if</span><span class="c4">&nbsp;precision </span><span class="c3">+</span><span class="c4">&nbsp;recall </span><span class="c3">==</span><span class="c4">&nbsp;0:<br> &nbsp; &nbsp; &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;0<br> &nbsp; &nbsp;</span><span class="c3">else</span><span class="c4">:<br> &nbsp; &nbsp; &nbsp; &nbsp;f1 </span><span class="c3">=</span><span class="c4">&nbsp;2 </span><span class="c3">*</span><span class="c4">&nbsp;(precision </span><span class="c3">*</span><span class="c4">&nbsp;recall) </span><span class="c3">/</span><span class="c4">&nbsp;(precision </span><span class="c3">+</span><span class="c4">&nbsp;recall)<br><br> &nbsp; &nbsp;print(f&quot;Accuracy: {accuracy}&quot;)<br> &nbsp; &nbsp;print(f&quot;Precision: {precision}&quot;)<br> &nbsp; &nbsp;print(f&quot;Recall: {recall}&quot;)<br> &nbsp; &nbsp;print(f&quot;F1 Score: {f1}&quot;)<br> &nbsp; &nbsp;print(f&quot;AUC: {auc}&quot;)<br><br></span><span class="c3">except</span><span class="c4">&nbsp;Exception </span><span class="c3">as</span><span class="c4">&nbsp;e:<br> &nbsp; &nbsp;print(f&quot;An error occurred during model evaluation: {e}&quot;)<br><br><br><br><br></span><span class="c4 c13"># Assuming `df_train` is your original training data and `df_generated` is the generated data</span><span class="c2"><br></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c9"><span class="c2"></span></p><p class="c14"><span class="c2"></span></p><p class="c11"><span class="c6">63/63 [==============================] - 0s 2ms/step<br>Epoch 1/10<br>2517/2518 [============================&gt;.] - ETA: 0s - loss: 45.0580 - label_output_loss: 9.8580 - attack_cat_output_loss: 35.1999 - label_output_accuracy: 0.8447 - attack_cat_output_accuracy: 0.7560<br>Epoch 1: attack_cat_output_loss improved from inf to 35.18869, saving model to best_gen_model.h5<br>2518/2518 [==============================] - 12s 4ms/step - loss: 45.0437 - label_output_loss: 9.8549 - attack_cat_output_loss: 35.1887 - label_output_accuracy: 0.8447 - attack_cat_output_accuracy: 0.7561<br>Epoch 2/10<br> &nbsp;36/2518 [..............................] - ETA: 7s - loss: 0.7701 - label_output_loss: 0.2543 - attack_cat_output_loss: 0.5158 - label_output_accuracy: 0.8672 - attack_cat_output_accuracy: 0.8064</span></p><p class="c9"><span class="c6"></span></p><p class="c14"><span class="c6"></span></p><p class="c11"><span class="c6">/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save(&#39;my_model.keras&#39;)`.<br> &nbsp;saving_api.save_model(<br></span></p><p class="c9"><span class="c6"></span></p><p class="c14"><span class="c6"></span></p><p class="c11"><span class="c6">2507/2518 [============================&gt;.] - ETA: 0s - loss: 1.2033 - label_output_loss: 0.3510 - attack_cat_output_loss: 0.8524 - label_output_accuracy: 0.8565 - attack_cat_output_accuracy: 0.7843<br>Epoch 2: attack_cat_output_loss improved from 35.18869 to 0.85101, saving model to best_gen_model.h5<br>2518/2518 [==============================] - 10s 4ms/step - loss: 1.2016 - label_output_loss: 0.3506 - attack_cat_output_loss: 0.8510 - label_output_accuracy: 0.8566 - attack_cat_output_accuracy: 0.7844<br>Epoch 3/10<br>2516/2518 [============================&gt;.] - ETA: 0s - loss: 0.8081 - label_output_loss: 0.2743 - attack_cat_output_loss: 0.5338 - label_output_accuracy: 0.8501 - attack_cat_output_accuracy: 0.7787<br>Epoch 3: attack_cat_output_loss improved from 0.85101 to 0.53396, saving model to best_gen_model.h5<br>2518/2518 [==============================] - 9s 3ms/step - loss: 0.8083 - label_output_loss: 0.2743 - attack_cat_output_loss: 0.5340 - label_output_accuracy: 0.8502 - attack_cat_output_accuracy: 0.7787<br>Epoch 4/10<br>2515/2518 [============================&gt;.] - ETA: 0s - loss: 0.8073 - label_output_loss: 0.2748 - attack_cat_output_loss: 0.5326 - label_output_accuracy: 0.8503 - attack_cat_output_accuracy: 0.7787<br>Epoch 4: attack_cat_output_loss improved from 0.53396 to 0.53260, saving model to best_gen_model.h5<br>2518/2518 [==============================] - 10s 4ms/step - loss: 0.8074 - label_output_loss: 0.2748 - attack_cat_output_loss: 0.5326 - label_output_accuracy: 0.8503 - attack_cat_output_accuracy: 0.7787<br>Epoch 5/10<br>2510/2518 [============================&gt;.] - ETA: 0s - loss: 0.8070 - label_output_loss: 0.2755 - attack_cat_output_loss: 0.5315 - label_output_accuracy: 0.8506 - attack_cat_output_accuracy: 0.7790<br>Epoch 5: attack_cat_output_loss improved from 0.53260 to 0.53151, saving model to best_gen_model.h5<br>2518/2518 [==============================] - 10s 4ms/step - loss: 0.8070 - label_output_loss: 0.2755 - attack_cat_output_loss: 0.5315 - label_output_accuracy: 0.8506 - attack_cat_output_accuracy: 0.7789<br>Epoch 6/10<br>2513/2518 [============================&gt;.] - ETA: 0s - loss: 0.8036 - label_output_loss: 0.2733 - attack_cat_output_loss: 0.5303 - label_output_accuracy: 0.8509 - attack_cat_output_accuracy: 0.7793<br>Epoch 6: attack_cat_output_loss improved from 0.53151 to 0.53029, saving model to best_gen_model.h5<br>2518/2518 [==============================] - 9s 4ms/step - loss: 0.8036 - label_output_loss: 0.2733 - attack_cat_output_loss: 0.5303 - label_output_accuracy: 0.8509 - attack_cat_output_accuracy: 0.7794<br>Epoch 7/10<br>2506/2518 [============================&gt;.] - ETA: 0s - loss: 0.8051 - label_output_loss: 0.2733 - attack_cat_output_loss: 0.5318 - label_output_accuracy: 0.8509 - attack_cat_output_accuracy: 0.7789<br>Epoch 7: attack_cat_output_loss did not improve from 0.53029<br>2518/2518 [==============================] - 9s 4ms/step - loss: 0.8052 - label_output_loss: 0.2733 - attack_cat_output_loss: 0.5319 - label_output_accuracy: 0.8509 - attack_cat_output_accuracy: 0.7789<br>Epoch 8/10<br>2511/2518 [============================&gt;.] - ETA: 0s - loss: 0.8037 - label_output_loss: 0.2734 - attack_cat_output_loss: 0.5303 - label_output_accuracy: 0.8509 - attack_cat_output_accuracy: 0.7793<br>Epoch 8: attack_cat_output_loss improved from 0.53029 to 0.53016, saving model to best_gen_model.h5<br>2518/2518 [==============================] - 11s 4ms/step - loss: 0.8035 - label_output_loss: 0.2733 - attack_cat_output_loss: 0.5302 - label_output_accuracy: 0.8509 - attack_cat_output_accuracy: 0.7793<br>Epoch 9/10<br>2507/2518 [============================&gt;.] - ETA: 0s - loss: 0.8189 - label_output_loss: 0.2749 - attack_cat_output_loss: 0.5440 - label_output_accuracy: 0.8506 - attack_cat_output_accuracy: 0.7790<br>Epoch 9: attack_cat_output_loss did not improve from 0.53016<br>2518/2518 [==============================] - 11s 4ms/step - loss: 0.8184 - label_output_loss: 0.2747 - attack_cat_output_loss: 0.5436 - label_output_accuracy: 0.8508 - attack_cat_output_accuracy: 0.7792<br>Epoch 10/10<br>2513/2518 [============================&gt;.] - ETA: 0s - loss: 0.8059 - label_output_loss: 0.2740 - attack_cat_output_loss: 0.5319 - label_output_accuracy: 0.8509 - attack_cat_output_accuracy: 0.7792<br>Epoch 10: attack_cat_output_loss did not improve from 0.53016<br>2518/2518 [==============================] - 10s 4ms/step - loss: 0.8058 - label_output_loss: 0.2740 - attack_cat_output_loss: 0.5318 - label_output_accuracy: 0.8509 - attack_cat_output_accuracy: 0.7793<br>An error occurred during model evaluation: in user code:<br><br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1972, in test_function &nbsp;*<br> &nbsp; &nbsp; &nbsp; &nbsp;return step_function(self, iterator)<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1956, in step_function &nbsp;**<br> &nbsp; &nbsp; &nbsp; &nbsp;outputs = model.distribute_strategy.run(run_step, args=(data,))<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1944, in run_step &nbsp;**<br> &nbsp; &nbsp; &nbsp; &nbsp;outputs = model.test_step(data)<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1852, in test_step<br> &nbsp; &nbsp; &nbsp; &nbsp;self.compute_loss(x, y, y_pred, sample_weight)<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py&quot;, line 1139, in compute_loss<br> &nbsp; &nbsp; &nbsp; &nbsp;return self.compiled_loss(<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py&quot;, line 265, in __call__<br> &nbsp; &nbsp; &nbsp; &nbsp;loss_value = loss_obj(y_t, y_p, sample_weight=sw)<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/losses.py&quot;, line 142, in __call__<br> &nbsp; &nbsp; &nbsp; &nbsp;losses = call_fn(y_true, y_pred)<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/losses.py&quot;, line 268, in call &nbsp;**<br> &nbsp; &nbsp; &nbsp; &nbsp;return ag_fn(y_true, y_pred, **self._fn_kwargs)<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/losses.py&quot;, line 2122, in categorical_crossentropy<br> &nbsp; &nbsp; &nbsp; &nbsp;return backend.categorical_crossentropy(<br> &nbsp; &nbsp;File &quot;/usr/local/lib/python3.10/dist-packages/keras/src/backend.py&quot;, line 5560, in categorical_crossentropy<br> &nbsp; &nbsp; &nbsp; &nbsp;target.shape.assert_is_compatible_with(output.shape)<br><br> &nbsp; &nbsp;ValueError: Shapes (None, 1) and (None, 8) are incompatible<br><br></span></p></body></html>